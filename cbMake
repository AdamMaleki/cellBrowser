#!/usr/bin/env python2

import logging, sys, optparse, struct, json, os, glob, shutil
from collections import defaultdict, namedtuple, OrderedDict
from os.path import join, basename, dirname, isfile, isdir

# directory to static data files, e.g. gencode tables
dataDir = join(dirname(__file__), "static")

# ==== functions =====
    
def parseArgs():
    " setup logging, parse command line arguments and options. -h shows auto-generated help page "
    parser = optparse.OptionParser("usage: %prog [options] outDir - copy all relevant js/css files into outDir, look for datasets in it and create index.html")

    parser.add_option("-d", "--debug", dest="debug", action="store_true",
        help="show debug messages")
    #parser.add_option("-m", "--meta", dest="meta", action="store",
        #help="meta data tsv file, aka sample sheet. One row per sample, first row has headers, first column has sample name."
        #)
    #parser.add_option("-e", "--matrix", dest="matrix", action="store",
        #help="expression matrix file, one gene per row, one sample per column. First column has gene identifiers (Ensembl or symbol), First row has sample names. ")
    #parser.add_option("-c", "--coords", dest="coords", action="append", help="tab-sep table with cell coordinates, format: metaId, x, y. Can be specified multiple times, if you have multiple coordinate files.")

    (options, args) = parser.parse_args()

    if args==[]:
        parser.print_help()
        exit(1)

    if options.debug:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    return args, options

def makeDir(outDir):
    if not isdir(outDir):
        logging.info("Creating %s" % outDir)
        os.makedirs(outDir)

def errAbort(msg):
        logging.error(msg)
        sys.exit(1)

def openFile(fname):
    if fname.endswith(".gz"):
        fh = gzip.open(fname)
    else:
        fh = open(fname)
    return fh

def parseDict(fname):
    """ parse text file in format key<tab>value and return as dict key->val """
    d = {}

    fh = openFile(fname)

    for line in fh:
        key, val = line.rstrip("\n").split("\t")
        d[key] = val
    return d

def findDatasets(outDir):
    """ search all subdirs of outDir for dataset.json files and return their contents as a list
    A dataset description is a list with three members: A label, the base URL and a longer description
    that can contain html.
    The attribute "priority" can be used to enforce an order on the datasets
    """
    datasets = []
    for subDir in os.listdir(outDir):
        if not isdir(join(outDir, subDir)):
            continue
        fname = join(outDir, subDir, "dataset.json")
        if not isfile(fname):
            continue

        datasetDesc = json.load(open(fname))
        assert("shortLabel" in datasetDesc)
        datasetDesc["baseUrl"] = subDir+"/"
        datasets.append(datasetDesc)
    datasets = list(sorted(datasets, key=lambda k: k.get('priority', 0)))
    logging.info("Found %d datasets" % len(datasets))
    return datasets

def copyAllFiles(fromDir, subDir, toDir):
    " copy all files in fromDir/subDir to toDir/subDir "
    outDir = join(toDir, subDir)
    makeDir(outDir)
    for filename in glob.glob(join(fromDir, subDir, '*')):
        if isdir(filename):
            continue
        logging.debug("Copying %s to %s" % (filename, outDir))
        shutil.copy(filename, outDir)

def copyStatic(baseDir, outDir):
    " copy all js, css and img files to outDir "
    logging.info("Copying js, css and img files to %s" % outDir)
    imgDir = join(outDir, "img")

    copyAllFiles(baseDir, "img", outDir)
    copyAllFiles(baseDir, "ext", outDir)
    copyAllFiles(baseDir, "js", outDir)
    copyAllFiles(baseDir, "css", outDir)

def makeIndexHtml(baseDir, datasets, outDir):
    dsList = []
    for ds in datasets:
        dsList.append({"shortLabel" : ds["shortLabel"], "sampleCount" : ds["sampleCount"], "name" : ds["name"] })

    indexFname = join(baseDir, "index.html")
    indexStr = open(indexFname).read()
    old = "datasetList = null"
    new = "datasetList = "+json.dumps(dsList, sort_keys=True, indent=4, separators=(',', ': '))
    newIndexStr = indexStr.replace(old, new)
    assert(newIndexStr!=indexStr)

    newFname = join(outDir, "index.html")
    ofh = open(newFname, "w")
    ofh.write(newIndexStr)
    ofh.close()

    datasetLabels = [x["name"] for x in dsList]
    logging.info("Wrote %s, added datasets: %s" % (newFname, " - ".join(datasetLabels)))

def cbMake(outDir, options):
    " create index.html in outDir and copy all statis files over "
    baseDir = dirname(__file__) # = directory of this script
    copyStatic(baseDir, outDir)
    datasets = findDatasets(outDir)
    makeIndexHtml(baseDir, datasets, outDir)

def main():
    args, options = parseArgs()
    outDir = args[0]

    if outDir is None:
        errAbort("You have to specify at least the output directory.")

    cbMake(outDir, options)

main()
