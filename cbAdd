#!/usr/bin/env python2

import logging, sys, optparse, struct, json, os, string, shutil, gzip, re, unicodedata
import zlib, math, operator, doctest, copy
from collections import defaultdict, namedtuple, OrderedDict, Counter
from os.path import join, basename, dirname, isfile, isdir, relpath, abspath

# I have made the crazy decision to make everything work without Numpy. Let's see how long this lasts
numpyLoaded = True
try:
    import numpy as np
except:
    numpyLoaded = False
    logging.warn("Numpy could not be loaded. The script should work, but it will be a lot slower to process the matrix.")

import time

# directory to static data files, e.g. gencode tables
dataDir = join(dirname(__file__), "static")

# ==== functions =====
    
def parseArgs(showHelp=False):
    " setup logging, parse command line arguments and options. -h shows auto-generated help page "
    parser = optparse.OptionParser("usage: %prog [options] -i dataset.conf -o outputDir - add a dataset to the single cell viewer directory")

    parser.add_option("-d", "--debug", dest="debug", action="store_true",
        help="show debug messages")
    #parser.add_option("-m", "--meta", dest="meta", action="store",
        #help="meta data tsv file, aka sample sheet. One row per sample, first row has headers, first column has sample name."
        #)
    #parser.add_option("-e", "--matrix", dest="matrix", action="store",
        #help="expression matrix file, one gene per row, one sample per column. First column has gene identifiers (Ensembl or symbol), First row has sample names. ")
    #parser.add_option("-c", "--coords", dest="coords", action="append", help="tab-sep table with cell coordinates, format: metaId, x, y. Can be specified multiple times, if you have multiple coordinate files.")

    parser.add_option("-i", "--inConf", dest="inConf", action="store",
        help="a dataset.conf file that specifies labels and all input files")
    parser.add_option("-o", "--outDir", dest="outDir", action="store", help="output directory")
    parser.add_option("-s", "--symTable",
        dest="symTable",
        action="store", help="use a table (transId, geneId, symbol) to convert transcript or gene identifiers in the expression matrix to symbols, default %default. Specify '-s none' if the input expression file is already using gene symbols.",
        default=join(dataDir, "gencode22.ens79-80.tab"))

    parser.add_option("-t", "--useTwoBytes",
        dest="useTwoBytes",
        action="store_true", help="use 2-byte-integers for the coordinates, default is to use four bytes")
    #parser.add_option("-e", "--enumFields",
        #dest="enumFields",
        #action="store", help="comma-sep list of meta fields that should be treated as categories, even when they contain only numbers. This is needed when you cluster ID field is a number, not a text string.")

    parser.add_option("-q", "--quick",
        dest="quick",
        action="store_true", help="Do not rebuild the big gene expression files, if they already exist")
    parser.add_option("", "--test",
        dest="test",
        action="store_true", help="run a few tests")

    #parser.add_option("-f", "--file", dest="file", action="store", help="run on file") 
    #parser.add_option("", "--test", dest="test", action="store_true", help="do something") 
    (options, args) = parser.parse_args()

    if showHelp:
        parser.print_help()
        exit(1)

    if options.debug:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    return args, options

def makeDir(outDir):
    if not isdir(outDir):
        logging.info("Creating %s" % outDir)
        os.makedirs(outDir)

def errAbort(msg, showHelp=False):
        logging.error(msg)
        if showHelp:
            parseArgs(showHelp=True)
        sys.exit(1)

def lineFileNextRow(inFile):
    """
    parses tab-sep file with headers in first line
    yields collection.namedtuples
    strips "#"-prefix from header line
    """

    if isinstance(inFile, str):
        fh = openFile(inFile)
    else:
        fh = inFile

    line1 = fh.readline()
    line1 = line1.strip("\n").lstrip("#")
    headers = line1.split("\t")
    headers = [re.sub("[^a-zA-Z0-9_]","_", h) for h in headers]
    headers = [re.sub("^_","", h) for h in headers] # remove _ prefix
    #headers = [x if x!="" else "noName" for x in headers]
    if headers[0]=="": # R does not name the first column by default
        headers[0]="rowName"

    if "" in headers:
        logging.error("Found empty cells in header line of %s" % inFile)
        logging.error("This often happens with Excel files. Make sure that the conversion from Excel was done correctly. Use cut -f-lastColumn to fix it.")
        assert(False)

    filtHeads = []
    for h in headers:
        if h[0].isdigit():
            filtHeads.append("x"+h)
        else:
            filtHeads.append(h)
    headers = filtHeads


    Record = namedtuple('tsvRec', headers)
    for line in fh:
        if line.startswith("#"):
            continue
        line = line.decode("latin1")
        # skip special chars in meta data and keep only ASCII
        line = unicodedata.normalize('NFKD', line).encode('ascii','ignore')
        line = line.rstrip("\n").rstrip("\r")
        fields = string.split(line, "\t", maxsplit=len(headers)-1)
        try:
            rec = Record(*fields)
        except Exception as msg:
            logging.error("Exception occured while parsing line, %s" % msg)
            logging.error("Filename %s" % fh.name)
            logging.error("Line was: %s" % line)
            logging.error("Does number of fields match headers?")
            logging.error("Headers are: %s" % headers)
            raise Exception("header count: %d != field count: %d wrong field count in line %s" % (len(headers), len(fields), line))
        yield rec

def parseOneColumn(fname, colName):
    " return a single column from a tsv as a list "
    ifh = open(fname)
    sep = "\t"
    headers = ifh.readline().rstrip("\n").rstrip("\r").split(sep)
    colIdx = headers.index(colName)
    vals = []
    for line in ifh:
        row = line.rstrip("\n").rstrip("\r").split(sep)
        vals.append(row[colIdx])
    return vals

def parseIntoColumns(fname):
    " parse tab sep file vertically, return as a list of (headerName, list of values) "
    ifh = open(fname)
    sep = "\t"
    headers = ifh.readline().rstrip("\n").rstrip("\r").split(sep)
    colsToGet = range(len(headers))

    columns = []
    for h in headers:
        columns.append([])

    for line in ifh:
        row = line.rstrip("\n").rstrip("\r").split(sep)
        for colIdx in colsToGet:
            columns[colIdx].append(row[colIdx])
    return zip(headers, columns)

def openFile(fname, mode="r"):
    if fname.endswith(".gz"):
        fh = gzip.open(fname, mode)
    else:
        fh = open(fname, mode)
    return fh

def parseDict(fname):
    """ parse text file in format key<tab>value and return as dict key->val """
    d = {}

    fh = openFile(fname)

    for line in fh:
        key, val = line.rstrip("\n").split("\t")
        d[key] = val
    return d

def readGeneToSym(fname):
    " given a file with geneId,symbol return a dict geneId -> symbol. Strips anything after . in the geneId "
    if fname.lower()=="none":
        return None

    logging.info("Reading gene,symbol mapping from %s" % fname)

    # Jim's files and CellRanger files have no headers, they are just key-value
    line1 = open(fname).readline()
    if "geneId" not in line1:
        d = parseDict(fname)
    # my new gencode tables contain a symbol for ALL genes
    elif line1=="transcriptId\tgeneId\tsymbol":
        for row in lineFileNextRow(fname):
            if row.symbol=="":
                continue
            d[row.geneId.split(".")[0]]=row.symbol
    # my own files have headers
    else:
        d = {}
        for row in lineFileNextRow(fname):
            if row.symbol=="":
                continue
            d[row.geneId.split(".")[0]]=row.symbol
    return d

def getDecilesList_np(values):
    deciles = np.percentile( values, [0,10,20,30,40,50,60,70,80,90,100] )
    return deciles

def bytesAndFmt(x):
    """ how many bytes do we need to store x values and what is the sprintf
    format string for it?
    """

    if x > 65535:
        assert(False) # field with more than 65k elements or high numbers? Weird meta data.

    if x > 255:
        return "Uint16", "<H" # see javascript typed array names, https://developer.mozilla.org/en-US/docs/Web/JavaScript/Typed_arrays
    else:
        return "Uint8", "<B"

#def getDecilesWithZeros(numVals):
#    """ return a pair of the deciles and their counts.
#    Counts is 11 elements long, the first element holds the number of zeros, 
#    which are treated separately
#
#    >>> l = [0,0,0,0,0,0,0,0,0,0,0,0,1,2,3,4,5,6,7,8,9,10]
#    >>> getDecilesWithZeros(l)
#     ([1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
#    """
#    nonZeros  = [x for x in numVals if x!=0.0]
#
#    zeroCount = len(numVals) - len(nonZeros)
#    deciles   = getDecilesList_np(nonZeros)
#
#    decArr = np.searchsorted(deciles, numVals)
#    decCounts(deciles, nonZeros)
#
#    decCounts.insert(0, zeroCount)
#    return deciles, decCounts, newVals

def discretizeField(numVals, fieldMeta, numType):
    " given a list of values, add attributes to fieldMeta that describe the deciles "
    #deciles, binCounts, newVals = getDecilesWithZeros(numVals)
    digArr, binInfo = digitizeArr(numVals, numType)
    deciles = []
    decCounts = []
    for start, end, count in binInfo:
        deciles.append(end)
        decCounts.append(count)

    fieldMeta["deciles"] = deciles
    fieldMeta["decCounts"] = decCounts
    fieldMeta["arrType"] = "uint8"
    fieldMeta["_fmt"] = "<B"
    return digArr, fieldMeta

def guessFieldMeta(valList, fieldMeta, colors, forceEnum):
    """ given a list of strings, determine if they're all int, float or
    strings. Return fieldMeta, as dict, and a new valList, with the correct python type
    - 'type' can be: 'int', 'float', 'enum' or 'uniqueString'
    - if int or float: 'deciles' is a list of the deciles
    - if uniqueString: 'maxLen' is the length of the longest string
    - if enum: 'values' is a list of all possible values
    - if colors is not None: 'colors' is a list of the default colors
    """
    intCount = 0
    floatCount = 0
    valCounts = defaultdict(int)
    maxVal = 0
    for val in valList:
        fieldType = "string"
        try:
            newVal = int(val)
            intCount += 1
            floatCount += 1
            maxVal = max(newVal, val)
        except:
            try:
                newVal = float(val)
                floatCount += 1
                maxVal = max(newVal, val)
            except:
                pass

        valCounts[val] += 1

    valToInt = None

    if floatCount==len(valList) and intCount!=len(valList) and len(valCounts) > 10 and not forceEnum:
        # field is a floating point number: convert to decile index
        numVals = [float(x) for x in valList]

        newVals, fieldMeta = discretizeField(numVals, fieldMeta, "float")

        fieldMeta["type"] = "float"
        fieldMeta["maxVal"] = maxVal

    elif intCount==len(valList) and not forceEnum:
        # field is an integer: convert to decile index
        numVals = [int(x) for x in valList]
        newVals, fieldMeta = discretizeField(numVals, fieldMeta, "int")
        fieldMeta["type"] = "int"
        fieldMeta["maxVal"] = maxVal

    elif len(valCounts)==len(valList) and not forceEnum:
        # field is a unique string
        fieldMeta["type"] = "uniqueString"
        maxLen = max([len(x) for x in valList])
        fieldMeta["maxSize"] = maxLen
        fieldMeta["_fmt"] = "%ds" % (maxLen+1)
        newVals = valList

    else:
        # field is an enum - convert to enum index
        fieldMeta["type"] = "enum"
        valArr = list(valCounts.keys())

        if colors!=None:
            colArr = []
            foundColors = 0
            notFound = set()
            for val in valArr:
                if val in colors:
                    colArr.append(colors[val])
                    foundColors +=1
                else:
                    notFound.add(val)
                    colArr.append("DDDDDD") # wonder if I should not stop here
            if foundColors > 0:
                fieldMeta["colors"] = colArr
                if len(notFound)!=0:
                    logging.warn("No default color found for field values %s" % notFound)

        fieldMeta["valCounts"] = list(sorted(valCounts.items(), key=operator.itemgetter(1), reverse=True))
        fieldMeta["arrType"], fieldMeta["_fmt"] = bytesAndFmt(len(valArr))
        valToInt = dict([(y,x) for (x,y) in enumerate(valArr)]) # dict with value -> index in valArr
        newVals = [valToInt[x] for x in valList]

    #fieldMeta["valCount"] = len(valList)
    fieldMeta["diffValCount"] = len(valCounts)

    return fieldMeta, newVals

def writeNum(col, packFmt, ofh):
    " write a list of numbers to a binary file "

def cleanString(s):
    " returns only alphanum characters in string s "
    newS = []
    for c in s:
        if c.isalnum():
            newS.append(c)
    return "".join(newS)

def metaToBin(fname, colorFname, outDir, enumFields, datasetInfo):
    """ convert meta table to binary files. outputs fields.json and one binary file per field. 
    adds names of metadata fields to datasetInfo and returns datasetInfo
    """
    makeDir(outDir)

    colData = parseIntoColumns(fname)

    colors = parseColors(colorFname)

    fieldInfo = []
    for colIdx, (fieldName, col) in enumerate(colData):
        logging.info("Meta data field index %d: '%s'" % (colIdx, fieldName))
        forceEnum = (fieldName in enumFields)
        cleanFieldName = cleanString(fieldName)
        binName = join(outDir, fieldName+".bin")

        fieldMeta = OrderedDict()
        fieldMeta["name"] = cleanFieldName
        fieldMeta["label"] = fieldName
        fieldMeta, binVals = guessFieldMeta(col, fieldMeta, colors, forceEnum)
        fieldType = fieldMeta["type"]

        packFmt = fieldMeta["_fmt"]

        # write the binary file
        binFh = open(binName, "wb")
        if fieldMeta["type"]!="uniqueString":
            for x in binVals:
                binFh.write(struct.pack(packFmt, x))
        else:
            for x in col:
                binFh.write("%s\n" % x)
        binFh.close()

        del fieldMeta["_fmt"]
        fieldInfo.append(fieldMeta)
        if "type" in fieldMeta:
            logging.info(("Type: %(type)s, %(diffValCount)d different values" % fieldMeta))
        else:
            logging.info(("Type: %(type)s, %(diffValCount)d different values, max size %(maxSize)d " % fieldMeta))

    datasetInfo["metaFields"] = fieldInfo
    #jsonFh.close()
    return datasetInfo

def iterLineOffsets(ifh):
    """ parse a text file and yield tuples of (line, startOffset, endOffset).
    endOffset does not include the newline, but the newline is not stripped from line.
    """
    line = True
    start = 0
    while line!='':
       line = ifh.readline()
       end = ifh.tell()-1
       if line!="":
           yield line, start, end
       start = ifh.tell()

def iterExprFromFile(fname, matType):
    " yield (gene, array) tuples from gene expression file "
    if fname.endswith(".gz"):
        ifh = gzip.open(fname)
    else:
        ifh = open(fname)
    
    sep = "\t"
    if ".csv" in fname.lower():
        sep = ","

    headLine = ifh.readline()
    for line in ifh:
        gene, rest = string.split(line, sep, maxsplit=1)
        if numpyLoaded:
            a = np.fromstring(rest, sep=sep, dtype=matType)
        else:
            a = rest.split(sep)
            if matType=="int":
                a = [int(x) for x in a]
            else:
                a = [float(x) for x in a]

        yield gene, a

def autoDetectMatType(matIter, n):
    " check if matrix has 'int' or 'float' data type by looking at the first n genes"
    # auto-detect the type of the matrix: int vs float
    geneCount = 0

    for gene, a in matIter:
        if numpyLoaded:
            a_int = a.astype(int)
            hasOnlyInts = np.array_equal(a, a_int)
            if not hasOnlyInts:
                return "float"
        else:
            for x in a:
                frac, whole = math.modf(x)
                if frac != 0.0:
                    return "float"
        if geneCount==n:
            break
        geneCount+=1
    return "int"

def getDecilesList(values):
    """ given a list of values, return the 10 values that define the 10 ranges for the deciles
    """
    if len(values)==0:
        return None

    valCount = len(values)
    binSize = float(valCount-1) / 10.0; # width of each bin, in number of elements, fractions allowed

    values = list(sorted(values))

    # get deciles from the list of sorted values
    deciles = []
    pos = 0
    for i in range(10): # 10 bins means that we need 10 limits, the last limit is at 90%
        pos = int (binSize * i)
        if pos > valCount: # this should not happen, but maybe it can, due to floating point issues?
            logging.warn("decile exceeds 10, binSize %d, i %d, len(values) %d" % (binSize, i, len(values)))
            pos = len(values)
        deciles.append ( values[pos] )
    return deciles

def findBin(ranges, val):
    """ given an array of values, find the index i where ranges[i] < val <= ranges[i+1]
    ranges have to be sorted.
    This is a dumb brute force implementation - maybe binary search is faster, if ever revisit this again
    Someone said up to 10 binary search is not faster.
    """
    if val==0: # speedup
        return 0
    for i in range(0, len(ranges)):
        if (val < ranges[i]):
            return i
    # if doesn't fit in anywhere, return beyond last possible index
    return i+1

def digitize_py(arr, matType):
    """ calculate deciles ignoring 0s from arr, use these deciles to digitize the whole arr,
    return (digArr, zeroCount, bins).
    bins is an array of (min, max, count)
    There are at most 11 bins and bin0 is just for the value zero.
    For bin0, min and max are both 0.0

    matType can be "int" or "float".
    If it is 'int' and arr has only <= 11 values, will not calculate deciles, but rather just
    count the numbers and use them to create bins, one per number.
    """
    if matType=="int":
        valCount = len(set(arr))
        if valCount <= 11: # 10 deciles + 0s
            counts = Counter(arr).most_common()
            counts.sort()

            valToIdx = {}
            for i, (val, count) in enumerate(counts):
                valToIdx[val] = i

            digArr = [valToIdx[x] for x in arr]
            bins = []
            for val, count in counts:
                bins.append( (val, val, count) )
            return digArr, bins

    noZeroArr = [x for x in arr if x!=0]
    zeroCount = len(arr) - len(noZeroArr)
    deciles = getDecilesList(noZeroArr) # there are 10 limits for the 10 deciles, 0% - 90%
    deciles.insert(0, 0) # bin0 is always for the zeros
    # we now have 11 limits
    assert(len(deciles)<=11)

    # digitize and count bins
    digArr = []
    binCounts = len(deciles)*[0]
    for x in arr:
        binIdx = findBin(deciles, x)
        # bin1 is always empty, so move down all other indices
        if binIdx>0:
            binIdx-=1
        digArr.append(binIdx)
        binCounts[binIdx]+=1

    # create the bin info
    bins = []
    bins.append( [float(0), float(0), float(binCounts[0])] )
    for i in range(1, len(deciles)):
        minVal = deciles[i-1]
        maxVal = deciles[i]
        count = binCounts[i]
        # skip empty bins
        if count!=0:
            bins.append( [float(minVal), float(maxVal), float(count)] )

    # add the maximum value explicitly, more meaningful
    bins[-1][1] = np.amax(arr)
    return digArr, bins

def digitizeArr(arr, numType):
    if numpyLoaded:
        return digitize_np(arr, numType)
    else:
        return digitize_py(arr, numType)

def binEncode(bins):
    " encode a list of at 11 three-tuples into a string of 33 floats (little endian)"
    # add (0,0,0) elements to bins until it has 11 elements "
    padBins = copy.copy(bins)
    for i in range(len(bins), 11):
        padBins.append( (0.0, 0.0, 0.0) )
    print len(padBins), padBins, len(padBins)
    assert(len(padBins)==11)

    strList = []
    for xMin, xMax, count in padBins:
        strList.append( struct.pack("<f", xMin) )
        strList.append( struct.pack("<f", xMax) )
        strList.append( struct.pack("<f", count) )
    ret = "".join(strList)
    assert(len(ret)==11*3*4)
    return ret

def digitize_np(arr, matType):
    " hopefully the same as digitize(), but using numpy "
    if matType=="int":
        # raw counts mode:
        # first try if there are enough unique values in the array
        # if there are <= 10 values, deciles make no sense,
        # so simply enumerate the values and map to bins 0-10
        binCounts = np.bincount(arr)
        nonZeroCounts = binCounts[np.nonzero(binCounts)] # remove the 0s
        if nonZeroCounts.size <= 11:
            valToBin = {}
            bins = []
            binIdx = 0
            for val, count in enumerate(binCounts):
                if count!=0:
                    bins.append( (val, val, count) )
                    valToBin[val] = binIdx
                    binIdx += 1
            # map values to bin indices, from stackoverflow
            digArr = np.vectorize(valToBin.__getitem__)(arr)
            return digArr, bins

    # calculate the deciles without the zeros, otherwise
    # the 0s completely distort the deciles
    noZero = np.copy(arr)
    noZero = arr[np.nonzero(arr)]

    deciles = np.percentile( noZero, [0,10,20,30,40,50,60,70,80,90] , interpolation="lower")
    # make sure that we always have a bin for the zeros
    deciles = np.insert(deciles, 0, 0)
    # now we have 10 limits, defining 11 bins
    # but bin1 will always be empty, as there is nothing between the value 0 and the lowest limit
    digArr = np.searchsorted(deciles, arr, side="right")
    # so we decrease all bin indices that are not 0
    np.putmask(digArr, digArr>0, digArr-1)
    binCounts = np.bincount(digArr)

    bins = []
    zeroCount = binCounts[0]

    # bin0 is a bit special
    bins.append( [float(0), float(0), zeroCount] )

    for i in range(1, len(deciles)):
        binCount = binCounts[i]
        if binCount==0:
            continue

        minVal = deciles[i-1]
        maxVal = deciles[i]
        bins.append( [minVal, maxVal, binCount] )

    bins[-1][1] = max(arr)
    return digArr, bins

def maxVal(a):
    if numpyLoaded:
        return np.amax(a)
    else:
        return max(a)

def matrixToBin(fname, geneToSym, binFname, jsonFname, skipLog):
    """ convert gene expression vectors to vectors of deciles
        and make json gene symbol -> (file offset, line length)
    """
    logging.info("converting %s to %s and writing index to %s" % (fname, binFname, jsonFname))
    logging.info("Shall expression values be log-transformed when transforming to deciles? -> %s" % (not skipLog))
    logging.info("Converting gene expression vectors to deciles...")
    ofh = open(binFname, "w")
    geneToOffset = {}
    skipIds = 0
    highCount = 0

    logging.info("Auto-detecting number type of %s" % fname)
    geneIter = iterExprFromFile(fname, "float")
    matType = autoDetectMatType(geneIter, 10)
    logging.info("Numbers in matrix are of type '%s'", matType)

    geneIter = iterExprFromFile(fname, matType)

    geneCount = 0
    for geneId, exprArr in geneIter:
        geneCount += 1
        if geneToSym is None:
            symbol = geneId
        else:
            symbol = geneToSym.get(geneId)
            if symbol is None:
                skipIds += 1
                logging.warn("%s is not a valid gene ID" % geneId)
                continue
        if symbol in geneToOffset:
            logging.warn("Gene %s/%s is duplicated in matrix, using only first occurence" % (geneId, symbol))
            continue

        if maxVal(exprArr) > 200:
            highCount += 1

        logging.debug("Processing %s, symbol %s" % (geneId, symbol))
        digArr, binInfo = digitizeArr(exprArr, matType)
        binStr = binEncode(binInfo)

        decChrList = [chr(x) for x in digArr]
        decStr = "".join(decChrList)
        geneIdLen = struct.pack("<H", len(geneId))

        # gene records are compressed.
        # The format of a record is:
        # - 2 bytes: length of descStr, e.g. gene identifier or else
        # - len(descStr) bytes: the descriptive string descStr
        # - 132 bytes: 11 deciles, encoded as 11 * 3 floats (=min, max, count)
        # - array of n bytes, n = number of cells
        geneStr = geneIdLen+geneId+binStr+decStr

        geneCompr = zlib.compress(geneStr)
        logging.debug("compression factor of %s: %f, before %d, after %d"% (symbol, float(len(geneCompr)) / len(geneStr), len(geneStr), len(geneCompr)))

        geneToOffset[symbol] = (ofh.tell(), len(geneCompr))
        ofh.write(geneCompr)

        if geneCount % 10 == 0:
            logging.info("Got deciles for %d genes" % geneCount)
    ofh.close()

    if highCount==0:
        logging.warn("No single value in the matrix is > 200. Has this matrix been log'ed?")
        #logging.error("Rerun with --skipLog.")
        #sys.exit(1)

    if len(geneToOffset)==0:
        errAbort("No genes from the expression matrix could be mapped to symbols in the input file."
            "Are you sure these are Ensembl IDs? If they are already gene symbols, re-run with '-s none'."
            "One example gene ID from the input matrix is %s." % geneId)

    if skipIds!=0:
        logging.warn("Could not resolve %d genes from Ensembl ID to gene symbol" % skipIds)

    ofh = open(jsonFname, "w")
    json.dump(geneToOffset, ofh)
    ofh.close()

def indexMatrix(fname, geneToSym, jsonFname):
    """ index a matrix with one gene per line and write json with a dict
        gene symbol -> (file offset, line length)
    """
    logging.info("Indexing line offsets of %s into %s" % (fname, jsonFname))
    ifh = open(fname)
    geneToOffset = {}
    skipIds = 0
    lineNo = 0
    for line, start, end in iterLineOffsets(ifh):
        if start == 0:
            symbol = "_header"
        else:
            geneId, _ = string.split(line, "\t", 1)
            if geneToSym is None:
                symbol = geneId
            else:
                symbol = geneToSym.get(geneId)
                if symbol is None:
                    skipIds += 1
                    continue
        lineLen = end - start
        assert(lineLen!=0)
        if symbol in geneToOffset:
            logging.warn("Gene %s/%s is duplicated in matrix, using only first occurence" % (geneId, symbol))
            continue
        geneToOffset[symbol] = (start, lineLen)
        lineNo += 1

    if len(geneToOffset)==1:
        errAbort("No genes could be mapped to symbols in the input file. Are you sure these are Ensembl IDs? If they are already gene symbols, re-run with '-s none'. One example gene ID is %s." % geneId)

    if skipIds!=0:
        logging.warn("Could not resolve %d genes from Ensembl ID to gene symbol" % skipIds)

    ofh = open(jsonFname, "w")
    json.dump(geneToOffset, ofh)
    ofh.close()

def indexMeta(fname, outFname):
    """ index a tsv by its first field. Writes binary data to outFname.
        binary data is (offset/4 bytes, line length/2 bytes)
    """
    ofh = open(outFname, "wb")
    logging.info("Indexing meta file %s to %s" % (fname, outFname))
    ifh = open(fname)
    headerDone = False
    for line, start, end in iterLineOffsets(ifh):
        if not headerDone:
            headerDone = True
            continue

        field1, _ = string.split(line, "\t", 1)
        lineLen = end - start
        assert(lineLen!=0)
        assert(lineLen<65535) # meta data line cannot be longer than 2 bytes
        ofh.write(struct.pack("<L", start))
        ofh.write(struct.pack("<H", lineLen))
    ofh.close()

def testMetaIndex(outDir):
    # test meta index
    fh = open(join(outDir, "meta.index"))
    #fh.seek(10*6)
    o = fh.read(4)
    s = fh.read(2)
    offset = struct.unpack("<L", o) # little endian
    l = struct.unpack("<H", s)
    print "offset, linelen:", offset, l

    #fh = open(join(outDir, "meta/meta.tsv"))
    #fh.seek(offset[0])
    #print fh.read(l[0])

# ----------- main --------------

def parseColors(fname):
    " parse color table and return as dict value -> color "
    if not isfile(fname):
        logging.warn("File %s does not exist" % fname)
        return None

    colDict = parseDict(fname)
    newDict = {}
    for metaVal, color in colDict.iteritems():
        color = color.strip().strip("#") # hbeale had a file with trailing spaces
        assert(len(color)<=6) # colors can be no more than six hex digits
        for c in color:
            assert(c in "0123456789ABCDEFabcdef") # color must be a hex number
        newDict[metaVal] = color
    return newDict

def parseScaleCoordsAsDict(fname, useTwoBytes):
    """ parse tsv file in format cellId, x, y and return as dict (cellId, x, y)
    Flip the y coordinates to make it more look like plots in R, for people transitioning from R.
    """
    logging.info("Parsing coordinates from %s" % fname)
    coords = []
    maxY = 0
    minX = 2^32
    minY = 2^32
    maxX = -2^32
    maxY = -2^32
    skipCount = 0
    for row in lineFileNextRow(fname):
        assert(len(row)==3) # coord file has to have three rows (cellId, x, y), we ignore the headers
        cellId = row[0]
        x = float(row[1])
        y = float(row[2])
        minX = min(x, minX)
        minY = min(y, minY)
        maxX = max(x, maxX)
        maxY = max(y, maxY)
        coords.append( (cellId, x, y) )

    if useTwoBytes:
        scaleX = 65535/(maxX-minX)
        scaleY = 65535/(maxY-minY)
        topY = 65535

    # not flipping coords anymore, origin is top-left ?
    newCoords = {}
    for cellId, x, y in coords:
        if useTwoBytes:
            #newX = int(scaleX * (maxX - x))
            #newY = int(65536 - (scaleY * (maxY - y)))
            x = int(scaleX * (x - minX))
            #newY = int(65536 - (scaleY * (y - minY)))
            y = int(scaleY * (y - minY))
            #print "XX", x, y, minX, minY, maxX, maxY, scaleX, scaleY, newX, newY
        else:
            #newY = maxY - y # just flip
            pass

        newCoords[cellId] = (x, y)

    return newCoords

def metaReorder(matrixFname, metaFname, fixedMetaFname):
    " check and reorder the meta data, has to be in the same order as the expression matrix, write to fixedMetaFname "
    logging.info("Checking and reordering meta data to %s" % fixedMetaFname)
    matrixSampleNames = readHeaders(matrixFname)[1:]
    metaSampleNames = [string.split(line, "\t", 1)[0] for line in open(metaFname)][1:]

    # check that there is a 1:1 sampleName relationship
    mat = set(matrixSampleNames)
    meta = set(metaSampleNames)
    if len(meta)!=len(metaSampleNames):
        logging.error("The meta data table contains a duplicate sample name")
        sys.exit(1)

    if len(mat.intersection(meta))==0:
        logging.error("Meta data and expression matrix have no single sample name in common")
        sys.exit(1)

    matNotMeta = meta - mat
    metaNotMat = mat - meta
    stop = False
    doFilter = False
    if len(metaNotMat)!=0:
        logging.warn("%d samples names are in the meta data, but not in the expression matrix. Examples: %s" % (len(metaNotMat), list(metaNotMat)[:10]))
        logging.warn("These samples will be removed from the meta data")
        matrixSampleNames = metaSampleNames
        doFilter = True

    if len(matNotMeta)!=0:
        logging.warn("%d samples names are in the expression matrix, but not in the meta data. Examples: %s" % (len(matNotMeta), list(matNotMeta)[:10]))
        logging.warn("These samples will be removed from the expression matrix")

    #if stop:
        #sys.exit(1)

    logging.info("Data contains %d samples/cells" % len(matrixSampleNames))

    # slurp in the whole meta data
    ofh = open(fixedMetaFname, "w")
    metaToLine = {}
    for lNo, line in enumerate(open(metaFname)):
        if lNo==0:
            ofh.write(line)
            continue
        row = line.rstrip("\n").rstrip("\r").split("\t")
        metaToLine[row[0]] = line

    # and spit it out in the right order
    for matrixName in matrixSampleNames:
        ofh.write(metaToLine[matrixName])
    ofh.close()

    return matrixSampleNames, doFilter

def writeCoords(coords, sampleNames, coordBin, coordJson, useTwoBytes, coordInfo):
    """ write coordinates given as a dictionary to coordBin and coordJson, in the order of sampleNames
    Also return as a list.
    """
    logging.info("Writing coordinates to %s and %s" % (coordBin, coordJson))
    binFh = open(coordBin, "wb")
    minX = 2^32
    minY = 2^32
    maxX = -2^32
    maxY = -2^32
    xVals = []
    yVals = []
    for sampleName in sampleNames:
        coordTuple = coords.get(sampleName)
        if coordTuple is None:
            errAbort("sample name %s is in coordinate file but not in matrix nor meta data")
        x, y = coordTuple
        minX = min(x, minX)
        minY = min(y, minY)
        maxX = max(x, maxX)
        maxY = max(y, maxY)

        # all little endian
        if useTwoBytes:
            binFh.write(struct.pack("<H", x))
            binFh.write(struct.pack("<H", y))
        else:
            binFh.write(struct.pack("<f", x))
            binFh.write(struct.pack("<f", y))

        xVals.append(x)
        yVals.append(y)
    
    coordInfo["minX"] = minX
    coordInfo["maxX"] = maxX
    coordInfo["minY"] = minY
    coordInfo["maxY"] = maxY
    if useTwoBytes:
        coordInfo["type"] = "Uint16"
    else:
        coordInfo["type"] = "Float32"
    return coordInfo, xVals, yVals

def copyMatrix(inFname, outFname, filtSampleNames, doFilter):
    " copy matrix and compress it. If doFilter is true: keep only the samples in filtSampleNames"
    if not doFilter:
        logging.info("Copying %s to %s" % (inFname, outFname))
        shutil.copy(inFname, outFname)
        if not outFname.endswith(".gz"):
            os.system("gzip %s" % outFname)
        return

    sep = "\t"

    logging.info("Copying %s to %s, keeping only the %d columns with a sample name in the meta data" % (inFname, outFname, len(filtSampleNames)))

    ifh = openFile(inFname)

    headLine = ifh.readline()
    headers = headLine.rstrip("\n").rstrip("\r").split(sep)

    keepFields = set(filtSampleNames)
    keepIdx = [0]
    for i, name in enumerate(headers):
        if name in keepFields:
            keepIdx.append(i)

    ofh = openFile(outFname, "w")


    for line in ifh:
        row = line.rstrip("\n").rstrip("\r").split(sep)
        newRow = []
        for idx in keepIdx:
            newRow.append(row[idx])
        ofh.write("\t".join(newRow))
        ofh.write("\n")
    ofh.close()

def convIdToSym(geneToSym, geneId):
    if geneToSym is None:
        return geneId
    else:
        return geneToSym[geneId]

def splitMarkerTable(filename, geneToSym, outDir):
    " split .tsv on first field and create many files in outDir with the non-first columns. Also map geneIds to symbols. "
    if filename is None:
        return
    logging.info("Splitting cluster markers from %s into directory %s" % (filename, outDir))
    #logging.debug("Splitting %s on first field" % filename)
    ifh = open(filename)

    headers = ifh.readline().rstrip("\n").split('\t')
    otherHeaders = headers[2:]

    data = defaultdict(list)
    for line in ifh:
        row = line.rstrip("\n").split('\t')
        markerName = row[0]
        geneId = row[1]
        scoreVal = float(row[2])
        otherFields = row[3:]

        geneSym = convIdToSym(geneToSym, geneId)

        newRow = []
        newRow.append(geneId)
        newRow.append(geneSym)
        newRow.append(scoreVal)
        newRow.extend(otherFields)

        data[markerName].append(newRow)

    newHeaders = ["id", "symbol"]
    newHeaders.extend(otherHeaders)

    fileCount = 0
    for markerName, rows in data.iteritems():
        #rows.sort(key=operator.itemgetter(2), reverse=True) # rev-sort by score (fold change)
        markerName = markerName.replace("/","_")
        outFname = join(outDir, markerName+".tsv")
        logging.debug("Writing %s" % outFname)
        ofh = open(outFname, "w")
        ofh.write("\t".join(newHeaders))
        ofh.write("\n")
        for row in rows:
            row[2] = "%0.3f" % row[2] # limit to 3 digits
            ofh.write("\t".join(row))
            ofh.write("\n")
        ofh.close()
        fileCount += 1
    logging.info("Wrote %d .tsv files into directory %s" % (fileCount, outDir))


def loadConfig(fname):
    " parse python in fname and return variables as dictionary "
    g = {}
    l = OrderedDict()
    execfile(fname, g, l)

    conf = l

    if not "coords" in conf:
        errAbort("The input configuration has to define the 'coords' statement")
    if not "meta" in conf:
        errAbort("The input configuration has to define the 'meta' statement")
    if not "exprMatrix" in conf:
        errAbort("The input configuration has to define the 'exprMatrix' statement")

    return conf

def guessConfig(options):
    " guess reasonable config options from arguments "
    conf = {}
    conf.name = dirname(options.matrix)

    #if options.inDir:
        #inDir = options.inDir
        #metaFname = join(inDir, "meta.tsv")
        #matrixFname = join(inDir, "exprMatrix.tsv")
        #coordFnames = [join(inDir, "tsne.coords.tsv")]
        #markerFname = join(inDir, "markers.tsv")
        #if isfile(markerFname):
            #markerFnames = [markerFname]
        #else:
            #markerFnames = None
#
        #acronymFname = join(inDir, "acronyms.tsv")
        #if isfile(acronymFname):
            #otherFiles["acronyms"] = [acronymFname]
#
        #markerFname = join(inDir, "markers.tsv")
        #if isfile(acronymFname):
            #otherFiles["markerLists"] = [markerFname]
    return conf

def mapFilenames(inDir, conf):
    # replace file names in config files with file names in the output directory
    copyFiles = []
    #copyFiles.append( (conf["meta"], "meta.tsv") )
    #conf["meta"] = "meta.tsv"

    #copyFiles.append( (conf["exprMatrix"], "exprMatrix.tsv") )
    #conf["exprMatrix"] = "exprMatrix.tsv"

    if "description" in conf:
        fname = makeAbs(inDir, conf["description"])
        if not isfile(fname):
            logging.error("%s referred to in config file does not exist, skipping" % fname)
            conf["description"] = None
        else:
            copyFiles.append( (fname, "description.html") )
            conf["description"] = "description.html"

    if "makeDoc" in conf:
        fname = makeAbs(inDir, conf["makeDoc"])
        if not isfile(fname):
            logging.error("%s referred to in config file does not exist, skipping" % fname)
            conf["makeDoc"] = None
        else:
            copyFiles.append( (fname, "makeDoc.html") )
            conf["makeDoc"] = "makeDoc.html"

    #if "colors" in conf:
        #fname = makeAbs(inDir, conf["colors"])
        #if not isfile(fname):
            #logging.error("%s referred to in config file does not exist, skipping" % fname)
            #conf["colors"] = None
        #else:
            #copyFiles.append( (fname, "colors.tsv") )
            #conf["colors"] = "colors.tsv"

    #newCoords = []
    #for coordIdx, (origFname, label) in enumerate(conf["coords"]):
        #newFname = "coords%d" % coordIdx
        #copyFiles.append( (origFname, newFname) )
        #newCoords.append( (newFname, label) )
    #conf["coords"] = newCoords

    #newMarkers = []
    #for coordIdx, (origFname, label) in enumerate(conf["markers"]):
        #newFname = "markers%d" % coordIdx
        #copyFiles.append( (origFname, newFname) )
        #newMarkers.append( (newFname, label) )
    #conf["markers"] = newMarkers

    return conf, copyFiles

def findInputFiles(options):
    """ find all input files and return them
    returns these:
    metaFname = file name meta data
    matrixFname = file name expression matrix
    coordFnames = list of (filename, label)
    markerFnames = list of (filename, label)
    filesToCopy = list of (origFname, copyToFname)
    """
    if options.inConf:
        conf = loadConfig(options.inConf)
        inDir = dirname(options.inConf)
    else:
        conf = guessConfig(options)

    # command line options can override the config file settings
    #if options.meta:
        #conf["meta"] = options.meta
    #if options.matrix:
        #conf["exprMatrix"] = options.matrix
    #if options.coords:
        #conf["coords"] = options.coords
    #if options.markers:
        #conf["markers"] = options.markers

    conf, copyFiles = mapFilenames(inDir, conf)

    return inDir, conf, copyFiles

def makeAbs(inDir, fname):
    " return absolute path of fname under inDir "
    return abspath(join(inDir, fname))

def makeAbsDict(inDir, dicts):
    " given list of dicts with key 'file', make paths absolute "
    for d in dicts:
        d["file"] = makeAbs(inDir, d["file"])
    return dicts

def parseTsvColumn(fname, colName):
    " parse a tsv file and return column as a pair (values, assignment row -> index in values) "
    logging.info("Parsing column %s from %s" % (colName, fname))
    vals = parseOneColumn(fname, colName)

    newVals = []
    valToInt = {}
    maxIdx = -1
    for v in vals:
        if v not in valToInt:
            maxIdx+=1
            valToInt[v] = maxIdx
        idx = valToInt[v]
        newVals.append(idx)

    intToVal = {v: k for k, v in valToInt.iteritems()} # inverse key/val in dict

    valArr = []
    for i in range(0, maxIdx+1):
        valArr.append(intToVal[i])

    return newVals, valArr

def makeMids(xVals, yVals, labelVec, labelVals):
    """
    calculate the positions (centers) for the cluster labels
    given a coord list and a vector of the same size with the label indeces, return a list of [x, y, coordLabel] 
    """
    logging.info("Calculating cluster midpoints")
    assert(len(xVals)==len(labelVec)==len(yVals))

    # prep the arrays
    clusterXVals = []
    clusterYVals = []
    for i in range(len(labelVals)):
        clusterXVals.append([])
        clusterYVals.append([])
    assert(len(clusterXVals)==len(labelVals))

    # sort the coords into separate arrays, one per cluster
    for i in range(len(labelVec)):
        #for (x, y), clusterIdx in zip(coords, labelVec):
        clusterIdx = labelVec[i]
        clusterXVals[clusterIdx].append(xVals[i])
        clusterYVals[clusterIdx].append(yVals[i])

    midInfo = []
    for clustIdx, xList in enumerate(clusterXVals):
        yList = clusterYVals[clustIdx]
        # get the midpoint
        midX = sum(xList) / float(len(xList))
        midY = sum(yList) / float(len(yList))

        # take only the best 70% of the points closest to the midpoints
        xyDist = []
        for x, y in zip(xList, yList):
            dist = math.sqrt((x-midX)**2+(y-midY)**2)
            xyDist.append( (dist, x, y) )
        xyDist.sort()
        xyDistBest = xyDist[:int(0.7*len(xyDist))]

        # now recalc the midpoint
        xSum = sum([x for dist, x, y in xyDistBest])
        ySum = sum([y for dist, x, y in xyDistBest])
        midX = xSum / float(len(xyDistBest))
        midY = ySum / float(len(xyDistBest))

        clusterName = labelVals[clustIdx]
        midInfo.append([midX, midY, clusterName])
    return midInfo

def readHeaders(fname):
    " return headers of a file "
    #return openFile(fname).readline().rstrip("\n").split("\t")
    return openFile(fname).readline().rstrip("\n").rstrip("\r").split("\t")

def addDataset(inDir, conf, fileToCopy, outDir, options):
    " write config to outDir and copy over all files in fileToCopy "
    # keep a copy of the original config
    confName = join(outDir, "origConf.json")
    json.dump(conf, open(confName, "w"))

    for src, dest in fileToCopy:
        logging.info("Copying %s -> %s" % (src, dest))
        shutil.copy(src, join(outDir, dest))

    matrixFname = makeAbs(inDir, conf["exprMatrix"])
    metaFname = makeAbs(inDir, conf["meta"])
    coordFnames = makeAbsDict(inDir, conf["coords"])
    markerFnames = makeAbsDict(inDir, conf["markers"])

    skipLog = conf["matrixIsLog"]
    colorFname = conf.get("colors")
    enumFields = conf.get("enumFields")

    # these don't exist / are not needed in the output json file
    del conf["meta"]
    del conf["exprMatrix"]
    del conf["matrixIsLog"]
    del conf["colors"]
    del conf["enumFields"]

    # index the meta data
    metaDir = join(outDir, "metaFields")
    makeDir(metaDir)
    metaIdxFname = join(outDir, "meta.index")

    finalMeta = join(outDir, "meta.tsv")
    if isfile(metaIdxFname) and options.quick:
        logging.info("quick-mode: %s already exists, not recreating" % metaIdxFname)
        sampleNames = readHeaders(finalMeta)[1:]
    else:
        sampleNames, needFilterMatrix = metaReorder(matrixFname, metaFname, finalMeta)
        conf["sampleCount"] = len(sampleNames)
        conf = metaToBin(metaFname, colorFname, metaDir, enumFields, conf)
        indexMeta(metaFname, metaIdxFname)

    # index the expression matrix
    geneToSym = readGeneToSym(options.symTable)

    myMatrixFname = join(outDir, "exprMatrix.tsv")
    if matrixFname.endswith(".gz"):
        myMatrixFname += ".gz"

    binMatFname = join(outDir, "exprMatrix.bin")
    matrixIndexFname = join(outDir, "exprMatrixOffsets.json")
    #indexMatrix(matrixFname, geneToSym, matrixIndexFname)

    if options.quick and isfile(matrixIndexFname):
        logging.info("quick-mode: Not discretizing matrix, because %s already exists" % matrixIndexFname)
    else:
        matrixToBin(matrixFname, geneToSym, binMatFname, matrixIndexFname, skipLog)

    if options.quick and isfile(myMatrixFname):
        logging.info("quick-mode: Not rebuilding expression matrix %s" % myMatrixFname)
    else:
        copyMatrix(matrixFname, myMatrixFname, sampleNames, needFilterMatrix)

    # convert the coordinates
    newCoords = []
    for coordIdx, coordInfo in enumerate(coordFnames):
        coordFname = coordInfo["file"]
        coordLabel = coordInfo["shortLabel"]
        coords = parseScaleCoordsAsDict(coordFname, options.useTwoBytes)
        #coordLabel = basename(coordFname).split(".")[0]
        coordName = "coords_%d" % coordIdx
        #coordName = cleanString(coordLabel)
        coordDir = join(outDir, "coords", coordName)
        makeDir(coordDir)
        coordBin = join(coordDir, "coords.bin")
        coordJson = join(coordDir, "coords.json")
        coordInfo = OrderedDict()
        coordInfo["name"] = coordName
        coordInfo["shortLabel"] = coordLabel
        coordInfo, xVals, yVals = writeCoords(coords, sampleNames, coordBin, coordJson, options.useTwoBytes, coordInfo)
        newCoords.append( coordInfo )
        conf["coords"] = newCoords

        if "labelField" in conf:
            clusterLabelField = conf["labelField"]
            labelVec, labelVals = parseTsvColumn(metaFname, clusterLabelField)
            clusterMids = makeMids(xVals, yVals, labelVec, labelVals)

            midFname = join(coordDir, "clusterLabels.json")
            midFh = open(midFname, "w")
            json.dump(clusterMids, midFh, indent=2)
            logging.info("Wrote cluster labels and midpoints to %s" % midFname)


    # convert the markers
    newMarkers = []
    for markerIdx, markerInfo in enumerate(markerFnames):
        markerFname = markerInfo["file"]
        markerLabel = markerInfo["shortLabel"]

        markerName = "markers_%d" % coordIdx # use sha1 of input file ?
        markerDir = join(outDir, "markers", markerName)
        makeDir(markerDir)

        splitMarkerTable(markerFname, geneToSym, markerDir)

        newMarkers.append( {"name" : markerName, "shortLabel" : markerLabel})
    conf["markers"] = newMarkers

    # write dataset summary info
    descJson = join(outDir, "dataset.json")
    descJsonFh = open(descJson, "w")
    json.dump(conf, descJsonFh, indent=2)
    logging.info("Wrote %s" % descJson)

def main():
    args, options = parseArgs()

    if options.test:
        #testMetaIndex(args[0])
        doctest.testmod()
        sys.exit(0)

    if options.inConf is None:
        errAbort("You have to specify at least an input config file.", showHelp=True)
    if options.outDir is None:
        errAbort("You have to specify at least the output directory.", showHelp=True)

    inDir, conf, filesToCopy = findInputFiles(options)

    outDir = options.outDir


    #if outDir is None or metaFname is None or matrixFname is None or len(coordFnames)==0:
        #errAbort("You have to specify at least a meta file, an expression matrix, a coordinate file  and one output directory")
    if outDir is None:
        errAbort("You have to specify at least the output directory.")

    datasetDir = join(outDir, conf["name"])
    makeDir(datasetDir)

    addDataset(inDir, conf, filesToCopy, datasetDir, options)

main()
