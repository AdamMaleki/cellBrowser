#!/usr/bin/env python2

import logging, sys, optparse, struct, json, os, string, shutil, gzip, re, unicodedata
import zlib, math, operator
from collections import defaultdict, namedtuple, OrderedDict
from os.path import join, basename, dirname, isfile, isdir, relpath, abspath

# directory to static data files, e.g. gencode tables
dataDir = join(dirname(__file__), "static")

# ==== functions =====
    
def parseArgs():
    " setup logging, parse command line arguments and options. -h shows auto-generated help page "
    parser = optparse.OptionParser("usage: %prog [options] -i dataset.conf -o outputDir - add a dataset to the single cell viewer")

    parser.add_option("-d", "--debug", dest="debug", action="store_true",
        help="show debug messages")
    #parser.add_option("-m", "--meta", dest="meta", action="store",
        #help="meta data tsv file, aka sample sheet. One row per sample, first row has headers, first column has sample name."
        #)
    #parser.add_option("-e", "--matrix", dest="matrix", action="store",
        #help="expression matrix file, one gene per row, one sample per column. First column has gene identifiers (Ensembl or symbol), First row has sample names. ")
    #parser.add_option("-c", "--coords", dest="coords", action="append", help="tab-sep table with cell coordinates, format: metaId, x, y. Can be specified multiple times, if you have multiple coordinate files.")

    parser.add_option("-i", "--inConf", dest="inConf", action="store",
        help="a dataset.conf file that specifies labels and all input files")
    parser.add_option("-o", "--outDir", dest="outDir", action="store", help="output directory")
    parser.add_option("-s", "--symTable",
        dest="symTable",
        action="store", help="use a table (transId, geneId, symbol) to convert transcript or gene identifiers in the expression matrix to symbols, default %default. Specify '-s none' if the input expression file is already using gene symbols.",
        default=join(dataDir, "gencode22.ens79-80.tab"))

    parser.add_option("-t", "--useTwoBytes",
        dest="useTwoBytes",
        action="store_true", help="use 2-byte-integers for the coordinates, default is to use four bytes")
    parser.add_option("-q", "--quick",
        dest="quick",
        action="store_true", help="Do not rebuild the big gene expression files, if they already exist")
    parser.add_option("", "--test",
        dest="test",
        action="store_true", help="run a few tests")

    #parser.add_option("-f", "--file", dest="file", action="store", help="run on file") 
    #parser.add_option("", "--test", dest="test", action="store_true", help="do something") 
    (options, args) = parser.parse_args()

    #if args==[]:
        #parser.print_help()
        #exit(1)

    if options.debug:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    return args, options

def makeDir(outDir):
    if not isdir(outDir):
        logging.info("Creating %s" % outDir)
        os.makedirs(outDir)

def errAbort(msg):
        logging.error(msg)
        sys.exit(1)

def lineFileNextRow(inFile):
    """
    parses tab-sep file with headers in first line
    yields collection.namedtuples
    strips "#"-prefix from header line
    """

    if isinstance(inFile, str):
        fh = openFile(inFile)
    else:
        fh = inFile

    line1 = fh.readline()
    line1 = line1.strip("\n").lstrip("#")
    headers = line1.split("\t")
    headers = [re.sub("[^a-zA-Z0-9_]","_", h) for h in headers]
    headers = [re.sub("^_","", h) for h in headers] # remove _ prefix
    #headers = [x if x!="" else "noName" for x in headers]
    if headers[0]=="": # R does not name the first column by default
        headers[0]="rowName"

    if "" in headers:
        logging.error("Found empty cells in header line of %s" % inFile)
        logging.error("This often happens with Excel files. Make sure that the conversion from Excel was done correctly. Use cut -f-lastColumn to fix it.")
        assert(False)

    filtHeads = []
    for h in headers:
        if h[0].isdigit():
            filtHeads.append("x"+h)
        else:
            filtHeads.append(h)
    headers = filtHeads


    Record = namedtuple('tsvRec', headers)
    for line in fh:
        if line.startswith("#"):
            continue
        line = line.decode("latin1")
        # skip special chars in meta data and keep only ASCII
        line = unicodedata.normalize('NFKD', line).encode('ascii','ignore')
        line = line.rstrip("\n").rstrip("\r")
        fields = string.split(line, "\t", maxsplit=len(headers)-1)
        try:
            rec = Record(*fields)
        except Exception as msg:
            logging.error("Exception occured while parsing line, %s" % msg)
            logging.error("Filename %s" % fh.name)
            logging.error("Line was: %s" % line)
            logging.error("Does number of fields match headers?")
            logging.error("Headers are: %s" % headers)
            raise Exception("header count: %d != field count: %d wrong field count in line %s" % (len(headers), len(fields), line))
        yield rec

def parseOneColumn(fname, colName):
    " return a single column from a tsv as a list "
    ifh = open(fname)
    sep = "\t"
    headers = ifh.readline().rstrip("\n").rstrip("\r").split(sep)
    colIdx = headers.index(colName)
    vals = []
    for line in ifh:
        row = line.rstrip("\n").rstrip("\r").split(sep)
        vals.append(row[colIdx])
    return vals

def parseIntoColumns(fname):
    " parse tab sep file vertically, return as a list of (headerName, list of values) "
    ifh = open(fname)
    sep = "\t"
    headers = ifh.readline().rstrip("\n").rstrip("\r").split(sep)
    colsToGet = range(len(headers))

    columns = []
    for h in headers:
        columns.append([])

    for line in ifh:
        row = line.rstrip("\n").rstrip("\r").split(sep)
        for colIdx in colsToGet:
            columns[colIdx].append(row[colIdx])
    return zip(headers, columns)

def openFile(fname, mode="r"):
    if fname.endswith(".gz"):
        fh = gzip.open(fname, mode)
    else:
        fh = open(fname, mode)
    return fh

def parseDict(fname):
    """ parse text file in format key<tab>value and return as dict key->val """
    d = {}

    fh = openFile(fname)

    for line in fh:
        key, val = line.rstrip("\n").split("\t")
        d[key] = val
    return d

def readGeneToSym(fname):
    " given a file with geneId,symbol return a dict geneId -> symbol. Strips anything after . in the geneId "
    if fname.lower()=="none":
        return None

    logging.info("Reading gene,symbol mapping from %s" % fname)

    # Jim's files and CellRanger files have no headers, they are just key-value
    line1 = open(fname).readline()
    if "geneId" not in line1:
        d = parseDict(fname)
    # my new gencode tables contain a symbol for ALL genes
    elif line1=="transcriptId\tgeneId\tsymbol":
        for row in lineFileNextRow(fname):
            if row.symbol=="":
                continue
            d[row.geneId.split(".")[0]]=row.symbol
    # my own files have headers
    else:
        d = {}
        for row in lineFileNextRow(fname):
            if row.symbol=="":
                continue
            d[row.geneId.split(".")[0]]=row.symbol
    return d

def getDecilesList(values):
    """ given a list of values, return the 11 values that define the 10 ranges for the deciles 
    >>> l = [-4.059,-3.644,-3.184,-3.184,-3.184,-3.059,-2.943,-2.396,-2.322,-2.252,-2.252,-2.252,-2.12,-2.12,-1.943,-1.943,-1.69,-1.556,-1.322,-1.184,-1.12,-0.862,-0.862,-0.837,-0.837,-0.667,-0.644,-0.535,-0.454,-0.234,-0.184,0.084,0.084,0.151,0.299,0.31,0.444,0.485,0.575,0.632,0.66,0.748,0.824,1.043,1.098,1.176,1.239,1.356,1.411,1.521,1.526,1.609,1.748,1.77,1.832,1.864,1.88,2.081,2.094,2.242,2.251,2.331,2.376,2.664,2.718,2.787,2.858,2.928,2.978,3.011,3.093,3.144,3.157,3.245,3.352,3.444,3.462,3.479,3.609,3.699,3.714,3.795,3.811,3.857,3.871,3.903,3.914,3.983,3.985,3.986,4.006,4.056,4.063,4.156,4.179,4.221,4.35,4.352,4.361,4.372,4.38,4.427,4.432,4.447,4.45,4.459,4.516,4.521,4.527,4.611,4.636,4.644,4.659,4.662,4.81,4.866,4.882,4.902,4.916,4.918,5.01,5.018,5.02,5.133,5.179,5.186,5.205,5.218,5.245,5.25,5.262,5.263,5.365,5.374,5.412,5.421,5.437,5.45,5.453,5.484,5.501,5.527,5.527,5.534,5.561,5.566,5.567,5.624,5.646,5.662,5.691,5.705,5.706,5.712,5.87,5.909,5.912,5.92,5.978,5.978,6.012,6.086,6.086,6.106,6.114,6.155,6.168,6.171,6.179,6.221,6.287,6.317,6.324,6.354,6.364,6.385,6.389,6.397,6.427,6.439,6.49,6.513,6.517,6.521,6.557,6.578,6.579,6.648,6.703,6.756,6.887,6.953,7.042,7.155,7.194,7.21,7.225,7.249,7.254,7.291,7.382,7.397,7.504,7.603,7.65,7.861,8.524]
    >>> getDecilesList(l)
    [-4.059, -1.12, 0.748, 2.331, 3.811, 4.447, 5.133, 5.561, 6.114, 6.578, 8.524]
    """
    if len(values)==0:
        return None

    valCount = len(values)
    binSize = float(valCount-1) / 10.0; # width of each bin, in number of elements, fractions allowed

    values = list(sorted(values))

    # get deciles from the list of sorted values
    deciles = []
    pos = 0
    for i in range(11): # 10 bins means that we need 11 limits
        pos = int (binSize * i)
        if pos > valCount: # this should not happen, but maybe it can, due to floating point issues?
            logging.warn("decile exceeds 10, binSize %d, i %d, len(values) %d" % (binSize, i, len(values)))
            pos = len(values)
        deciles.append ( values[pos] )
    return deciles

def bytesAndFmt(x):
    """ how many bytes do we need to store x values and what is the sprintf
    format string for it?
    """

    if x > 65535:
        assert(False) # field with more than 65k elements or high numbers? Weird meta data.

    if x > 255:
        return "Uint16", "<H" # see javascript typed array names, https://developer.mozilla.org/en-US/docs/Web/JavaScript/Typed_arrays
    else:
        return "Uint8", "<B"

def discretizeField(numVals, fieldMeta):
    " given a list of values, add attributes to fieldMeta that describe the deciles "
    deciles = getDecilesList(numVals)
    binCounts = [0]*10
    newVals = []
    for x in numVals:
        binIdx = findBin(deciles, x)
        newVals.append(binIdx)
        binCounts[binIdx]+=1

    fieldMeta["deciles"] = deciles
    fieldMeta["binCounts"] = binCounts
    fieldMeta["arrType"] = "uint8"
    fieldMeta["_fmt"] = "<B"
    return newVals, fieldMeta

def guessFieldMeta(valList, fieldMeta, colors):
    """ given a list of strings, determine if they're all int, float or
    strings. Return fieldMeta, as dict, and a new valList, with the correct python type
    - 'type' can be: 'int', 'float', 'enum' or 'uniqueString'
    - if int or float: 'deciles' is a list of the deciles
    - if uniqueString: 'maxLen' is the length of the longest string
    - if enum: 'values' is a list of all possible values
    - if colors is not None: 'colors' is a list of the default colors
    """
    intCount = 0
    floatCount = 0
    valCounts = defaultdict(int)
    maxVal = 0
    for val in valList:
        fieldType = "string"
        try:
            newVal = int(val)
            intCount += 1
            floatCount += 1
            maxVal = max(newVal, val)
        except:
            try:
                newVal = float(val)
                floatCount += 1
                maxVal = max(newVal, val)
            except:
                pass

        valCounts[val] += 1

    valToInt = None

    if floatCount==len(valList) and intCount!=len(valList) and len(valCounts) > 10:
        # field is a floating point number: convert to decile index
        numVals = [float(x) for x in valList]
        newVals, fieldMeta = discretizeField(numVals, fieldMeta)
        fieldMeta["type"] = "float"
        fieldMeta["maxVal"] = maxVal

    elif intCount==len(valList):
        # field is an integer: convert to decile index
        numVals = [int(x) for x in valList]
        newVals, fieldMeta = discretizeField(numVals, fieldMeta)
        fieldMeta["type"] = "int"
        fieldMeta["maxVal"] = maxVal

    elif len(valCounts)==len(valList):
        # field is a unique string
        fieldMeta["type"] = "uniqueString"
        maxLen = max([len(x) for x in valList])
        fieldMeta["maxSize"] = maxLen
        fieldMeta["_fmt"] = "%ds" % (maxLen+1)
        newVals = valList

    else:
        # field is an enum - convert to enum index
        fieldMeta["type"] = "enum"
        valArr = list(valCounts.keys())

        if colors!=None:
            colArr = []
            foundColors = 0
            notFound = set()
            for val in valArr:
                if val in colors:
                    colArr.append(colors[val])
                    foundColors +=1
                else:
                    notFound.add(val)
                    colArr.append("DDDDDD") # wonder if I should not stop here
            if foundColors > 0:
                fieldMeta["colors"] = colArr
                if len(notFound)!=0:
                    logging.warn("No default color found for field values %s" % notFound)

        fieldMeta["valCounts"] = list(sorted(valCounts.items(), key=operator.itemgetter(1), reverse=True))
        fieldMeta["arrType"], fieldMeta["_fmt"] = bytesAndFmt(len(valArr))
        valToInt = dict([(y,x) for (x,y) in enumerate(valArr)]) # dict with value -> index in valArr
        newVals = [valToInt[x] for x in valList]

    #fieldMeta["valCount"] = len(valList)
    fieldMeta["diffValCount"] = len(valCounts)

    return fieldMeta, newVals

def writeNum(col, packFmt, ofh):
    " write a list of numbers to a binary file "

def findBin(ranges, val):
    """ given an array of values, find the index i where ranges[i] < val < ranges[i+1]
    This is a dumb brute force implementation - use binary search?
    """
    for i in range(0, len(ranges)):
        if ((val >= ranges[i]) and (val <= ranges[i+1])):
            return i

def cleanString(s):
    " returns only alphanum characters in string s "
    newS = []
    for c in s:
        if c.isalnum():
            newS.append(c)
    return "".join(newS)

def metaToBin(fname, colorFname, outDir, datasetInfo):
    """ convert meta table to binary files. outputs fields.json and one binary file per field. 
    adds names of metadata fields to datasetInfo and returns datasetInfo
    """
    makeDir(outDir)

    colData = parseIntoColumns(fname)

    colors = parseColors(colorFname)

    fieldInfo = []
    for colIdx, (fieldName, col) in enumerate(colData):
        logging.info("Meta data field index %d: '%s'" % (colIdx, fieldName))
        cleanFieldName = cleanString(fieldName)
        binName = join(outDir, fieldName+".bin")

        fieldMeta = OrderedDict()
        fieldMeta["name"] = cleanFieldName
        fieldMeta["label"] = fieldName
        fieldMeta, binVals = guessFieldMeta(col, fieldMeta, colors)
        fieldType = fieldMeta["type"]

        packFmt = fieldMeta["_fmt"]

        # write the binary file
        binFh = open(binName, "wb")
        if fieldMeta["type"]!="uniqueString":
            for x in binVals:
                binFh.write(struct.pack(packFmt, x))
        else:
            for x in col:
                binFh.write("%s\n" % x)
        binFh.close()

        del fieldMeta["_fmt"]
        fieldInfo.append(fieldMeta)
        if "type" in fieldMeta:
            logging.info(("Type: %(type)s, %(diffValCount)d different values" % fieldMeta))
        else:
            logging.info(("Type: %(type)s, %(diffValCount)d different values, max size %(maxSize)d " % fieldMeta))

    datasetInfo["metaFields"] = fieldInfo
    #jsonFh.close()
    return datasetInfo

def iterLineOffsets(ifh):
    """ parse a text file and yield tuples of (line, startOffset, endOffset).
    endOffset does not include the newline, but the newline is not stripped from line.
    """
    line = True
    start = 0
    while line!='':
       line = ifh.readline()
       end = ifh.tell()-1
       if line!="":
           yield line, start, end
       start = ifh.tell()

def matrixToBin(fname, geneToSym, binFname, jsonFname, skipLog):
    """ convert gene expression vectors to vectors of deciles
        and make json gene symbol -> (file offset, line length)
    """
    logging.info("converting %s to %s and writing index to %s" % (fname, binFname, jsonFname))
    logging.info("Shall expression values be log-transformed when transforming to deciles? -> %s" % (not skipLog))
    logging.info("Converting gene expression vectors to deciles...")
    ofh = open(binFname, "w")
    geneToOffset = {}
    skipIds = 0
    highCount = 0

    ifh = openFile(fname)

    headers = ifh.readline().rstrip("\n").split("\t")

    geneCount = 0
    for line in ifh:
        geneCount += 1
        row = line.split("\t")
        geneId = row[0]
        if geneToSym is None:
            symbol = geneId
        else:
            symbol = geneToSym.get(geneId)
            if symbol is None:
                skipIds += 1
                continue
        if symbol in geneToOffset:
            logging.warn("Gene %s/%s is duplicated in matrix, using only first occurence" % (geneId, symbol))
            continue

        vals = [float(x) for x in row[1:]]
        if max(vals) > 500:
            highCount += 1

        # TODO: slow - port this to numpy, but keep this old code to avoid numpy dependency
	if not skipLog:
            vals = [math.log(x+1, 2) for x in vals]
        deciles = getDecilesList(vals) # should we not rather remove 0-values before doing this?
        decVals = [findBin(deciles, x) for x in vals]
        # TODO end

        decChrList = [chr(x) for x in decVals]
        decStr = "".join(decChrList)

        decCompr = zlib.compress(decStr)
        logging.debug("compression factor of %s: %f, before %d, after %d"% (symbol, float(len(decCompr)) / len(decStr), len(decStr), len(decCompr)))

        geneToOffset[symbol] = (ofh.tell(), len(decCompr), deciles)
        ofh.write(decCompr)

        if geneCount % 1000 == 0:
            logging.info("Got deciles for %d genes" % geneCount)
    ofh.close()

    if highCount==0:
        logging.error("No single value in the matrix is > 500. It looks like this matrix has been log'ed already")
        logging.error("Rerun with --skipLog.")
        sys.exit(1)

    if len(geneToOffset)==0:
        errAbort("No genes from the expression matrix could be mapped to symbols in the input file."
            "Are you sure these are Ensembl IDs? If they are already gene symbols, re-run with '-s none'."
            "One example gene ID from the input matrix is %s." % geneId)

    if skipIds!=0:
        logging.warn("Could not resolve %d genes from Ensembl ID to gene symbol" % skipIds)

    ofh = open(jsonFname, "w")
    json.dump(geneToOffset, ofh)
    ofh.close()

def indexMatrix(fname, geneToSym, jsonFname):
    """ index a matrix with one gene per line and write json with a dict
        gene symbol -> (file offset, line length)
    """
    logging.info("Indexing line offsets of %s into %s" % (fname, jsonFname))
    ifh = open(fname)
    geneToOffset = {}
    skipIds = 0
    lineNo = 0
    for line, start, end in iterLineOffsets(ifh):
        if start == 0:
            symbol = "_header"
        else:
            geneId, _ = string.split(line, "\t", 1)
            if geneToSym is None:
                symbol = geneId
            else:
                symbol = geneToSym.get(geneId)
                if symbol is None:
                    skipIds += 1
                    continue
        lineLen = end - start
        assert(lineLen!=0)
        if symbol in geneToOffset:
            logging.warn("Gene %s/%s is duplicated in matrix, using only first occurence" % (geneId, symbol))
            continue
        geneToOffset[symbol] = (start, lineLen)
        lineNo += 1

    if len(geneToOffset)==1:
        errAbort("No genes could be mapped to symbols in the input file. Are you sure these are Ensembl IDs? If they are already gene symbols, re-run with '-s none'. One example gene ID is %s." % geneId)

    if skipIds!=0:
        logging.warn("Could not resolve %d genes from Ensembl ID to gene symbol" % skipIds)

    ofh = open(jsonFname, "w")
    json.dump(geneToOffset, ofh)
    ofh.close()

def indexMeta(fname, outFname):
    """ index a tsv by its first field. Writes binary data to outFname.
        binary data is (offset/4 bytes, line length/2 bytes)
    """
    ofh = open(outFname, "wb")
    logging.info("Indexing meta file %s to %s" % (fname, outFname))
    ifh = open(fname)
    headerDone = False
    for line, start, end in iterLineOffsets(ifh):
        if not headerDone:
            headerDone = True
            continue

        field1, _ = string.split(line, "\t", 1)
        lineLen = end - start
        assert(lineLen!=0)
        assert(lineLen<65535) # meta data line cannot be longer than 2 bytes
        ofh.write(struct.pack("<L", start))
        ofh.write(struct.pack("<H", lineLen))
    ofh.close()

def testMetaIndex(outDir):
    # test meta index
    fh = open(join(outDir, "meta.index"))
    #fh.seek(10*6)
    o = fh.read(4)
    s = fh.read(2)
    offset = struct.unpack("<L", o) # little endian
    l = struct.unpack("<H", s)
    print "offset, linelen:", offset, l

    #fh = open(join(outDir, "meta/meta.tsv"))
    #fh.seek(offset[0])
    #print fh.read(l[0])

# ----------- main --------------

def parseColors(fname):
    " parse color table and return as dict value -> color "
    if not isfile(fname):
        logging.warn("File %s does not exist" % fname)
        return None

    colDict = parseDict(fname)
    newDict = {}
    for metaVal, color in colDict.iteritems():
        color = color.strip().strip("#") # hbeale had a file with trailing spaces
        assert(len(color)<=6) # colors can be no more than six hex digits
        for c in color:
            assert(c in "0123456789ABCDEFabcdef") # color must be a hex number
        newDict[metaVal] = color
    return newDict

def parseScaleCoordsAsDict(fname, useTwoBytes):
    """ parse tsv file in format cellId, x, y and return as dict (cellId, x, y)
    Flip the y coordinates to make it more look like plots in R, for people transitioning from R.
    """
    logging.info("Parsing coordinates from %s" % fname)
    coords = []
    maxY = 0
    minX = 2^32
    minY = 2^32
    maxX = -2^32
    maxY = -2^32
    skipCount = 0
    for row in lineFileNextRow(fname):
        assert(len(row)==3) # coord file has to have three rows (cellId, x, y), we ignore the headers
        cellId = row[0]
        x = float(row[1])
        y = float(row[2])
        minX = min(x, minX)
        minY = min(y, minY)
        maxX = max(x, maxX)
        maxY = max(y, maxY)
        coords.append( (cellId, x, y) )

    if useTwoBytes:
        scaleX = (maxX-minX)/65536
        scaleY = (maxY-minY)/65536
        topY = 65535

    # not flipping coords anymore, origin is top-left ?
    newCoords = {}
    for cellId, x, y in coords:
        if useTwoBytes:
            #newX = int(scaleX * (maxX - x))
            #newY = int(65536 - (scaleY * (maxY - y)))
            newX = int(scaleX * (x - minX))
            newY = int(65536 - (scaleY * (y - minY)))
        else:
            #newY = maxY - y # just flip
            newY = y # just flip

        newCoords[cellId] = (x, newY)

    return newCoords

def metaReorder(matrixFname, metaFname, fixedMetaFname):
    " check and reorder the meta data, has to be in the same order as the expression matrix, write to fixedMetaFname "
    logging.info("Checking and reordering meta data to %s" % fixedMetaFname)
    matrixSampleNames = openFile(matrixFname).readline().rstrip("\n").rstrip("\r").split("\t")[1:]
    metaSampleNames = [string.split(line, "\t", 1)[0] for line in open(metaFname)][1:]

    # check that there is a 1:1 sampleName relationship
    mat = set(matrixSampleNames)
    meta = set(metaSampleNames)
    if len(meta)!=len(metaSampleNames):
        logging.error("The meta data table contains a duplicate sample name")
        sys.exit(1)

    if len(mat.intersection(meta))==0:
        logging.error("Meta data and expression matrix have no single sample name in common")
        sys.exit(1)

    matNotMeta = meta - mat
    metaNotMat = mat - meta
    stop = False
    doFilter = False
    if len(metaNotMat)!=0:
        logging.warn("%d samples names are in the meta data, but not in the expression matrix. Examples: %s" % (len(metaNotMat), list(metaNotMat)[:10]))
        logging.warn("These samples will be removed from the meta data")
        matrixSampleNames = metaSampleNames
        doFilter = True

    if len(matNotMeta)!=0:
        logging.warn("%d samples names are in the expression matrix, but not in the meta data. Examples: %s" % (len(matNotMeta), list(matNotMeta)[:10]))
        logging.warn("These samples will be removed from the expression matrix")

    #if stop:
        #sys.exit(1)

    logging.info("Data contains %d samples/cells" % len(matrixSampleNames))

    # slurp in the whole meta data
    ofh = open(fixedMetaFname, "w")
    metaToLine = {}
    for lNo, line in enumerate(open(metaFname)):
        if lNo==0:
            ofh.write(line)
            continue
        row = line.rstrip("\n").rstrip("\r").split("\t")
        metaToLine[row[0]] = line

    # and spit it out in the right order
    for matrixName in matrixSampleNames:
        ofh.write(metaToLine[matrixName])
    ofh.close()

    return matrixSampleNames, doFilter

def writeCoords(coords, sampleNames, coordBin, coordJson, useTwoBytes, coordInfo):
    """ write coordinates given as a dictionary to coordBin and coordJson, in the order of sampleNames
    Also return as a list.
    """
    logging.info("Writing coordinates to %s and %s" % (coordBin, coordJson))
    binFh = open(coordBin, "wb")
    minX = 2^32
    minY = 2^32
    maxX = -2^32
    maxY = -2^32
    xVals = []
    yVals = []
    for sampleName in sampleNames:
        coordTuple = coords.get(sampleName)
        if coordTuple is None:
            errAbort("sample name %s is in coordinate file but not in matrix nor meta data")
        x, y = coordTuple
        minX = min(x, minX)
        minY = min(y, minY)
        maxX = max(x, maxX)
        maxY = max(y, maxY)

        # all little endian
        if useTwoBytes:
            binFh.write(struct.pack("<H", x))
            binFh.write(struct.pack("<H", y))
        else:
            binFh.write(struct.pack("<f", x))
            binFh.write(struct.pack("<f", y))

        xVals.append(x)
        yVals.append(y)
    
    coordInfo["minX"] = minX
    coordInfo["maxX"] = maxX
    coordInfo["minY"] = minY
    coordInfo["maxY"] = maxY
    if useTwoBytes:
        coordInfo["type"] = "Uint16"
    else:
        coordInfo["type"] = "Float32"
    return coordInfo, xVals, yVals

def copyMatrix(inFname, outFname, filtSampleNames, doFilter):
    " copy matrix and compress it. If doFilter is true: keep only the samples in filtSampleNames"
    if not doFilter:
        logging.info("Copying %s to %s" % (inFname, outFname))
        shutil.copy(inFname, outFname)
        if not outFname.endswith(".gz"):
            os.system("gzip %s" % outFname)
        return

    sep = "\t"

    logging.info("Copying %s to %s, keeping only the %d columns with a sample name in the meta data" % (inFname, outFname, len(filtSampleNames)))

    ifh = openFile(inFname)

    headLine = ifh.readline()
    headers = headLine.rstrip("\n").rstrip("\r").split(sep)

    keepFields = set(filtSampleNames)
    keepIdx = [0]
    for i, name in enumerate(headers):
        if name in keepFields:
            keepIdx.append(i)

    ofh = openFile(outFname, "w")


    for line in ifh:
        row = line.rstrip("\n").rstrip("\r").split(sep)
        newRow = []
        for idx in keepIdx:
            newRow.append(row[idx])
        ofh.write("\t".join(newRow))
        ofh.write("\n")
    ofh.close()

def convIdToSym(geneToSym, geneId):
    if geneToSym is None:
        return geneId
    else:
        return geneToSym[geneId]

def splitMarkerTable(filename, geneToSym, outDir):
    " split .tsv on first field and create many files in outDir with the non-first columns. Also map geneIds to symbols. "
    if filename is None:
        return
    logging.info("Splitting cluster markers from %s into directory %s" % (filename, outDir))
    #logging.debug("Splitting %s on first field" % filename)
    ifh = open(filename)

    headers = ifh.readline().rstrip("\n").split('\t')
    otherHeaders = headers[2:]

    data = defaultdict(list)
    for line in ifh:
        row = line.rstrip("\n").split('\t')
        markerName = row[0]
        geneId = row[1]
        scoreVal = float(row[2])
        otherFields = row[3:]

        geneSym = convIdToSym(geneToSym, geneId)

        newRow = []
        newRow.append(geneId)
        newRow.append(geneSym)
        newRow.append(scoreVal)
        newRow.extend(otherFields)

        data[markerName].append(newRow)

    newHeaders = ["id", "symbol"]
    newHeaders.extend(otherHeaders)

    fileCount = 0
    for markerName, rows in data.iteritems():
        #rows.sort(key=operator.itemgetter(2), reverse=True) # rev-sort by score (fold change)
        markerName = markerName.replace("/","_")
        outFname = join(outDir, markerName+".tsv")
        logging.debug("Writing %s" % outFname)
        ofh = open(outFname, "w")
        ofh.write("\t".join(newHeaders))
        ofh.write("\n")
        for row in rows:
            row[2] = "%0.3f" % row[2] # limit to 3 digits
            ofh.write("\t".join(row))
            ofh.write("\n")
        ofh.close()
        fileCount += 1
    logging.info("Wrote %d .tsv files into directory %s" % (fileCount, outDir))


def loadConfig(fname):
    " parse python in fname and return variables as dictionary "
    g = {}
    l = OrderedDict()
    execfile(fname, g, l)

    conf = l

    if not "coords" in conf:
        errAbort("The input configuration has to define the 'coords' statement")
    if not "meta" in conf:
        errAbort("The input configuration has to define the 'meta' statement")
    if not "exprMatrix" in conf:
        errAbort("The input configuration has to define the 'exprMatrix' statement")

    return conf

def guessConfig(options):
    " guess reasonable config options from arguments "
    conf = {}
    conf.name = dirname(options.matrix)

    #if options.inDir:
        #inDir = options.inDir
        #metaFname = join(inDir, "meta.tsv")
        #matrixFname = join(inDir, "exprMatrix.tsv")
        #coordFnames = [join(inDir, "tsne.coords.tsv")]
        #markerFname = join(inDir, "markers.tsv")
        #if isfile(markerFname):
            #markerFnames = [markerFname]
        #else:
            #markerFnames = None
#
        #acronymFname = join(inDir, "acronyms.tsv")
        #if isfile(acronymFname):
            #otherFiles["acronyms"] = [acronymFname]
#
        #markerFname = join(inDir, "markers.tsv")
        #if isfile(acronymFname):
            #otherFiles["markerLists"] = [markerFname]
    return conf

def mapFilenames(inDir, conf):
    # replace file names in config files with file names in the output directory
    copyFiles = []
    #copyFiles.append( (conf["meta"], "meta.tsv") )
    #conf["meta"] = "meta.tsv"

    #copyFiles.append( (conf["exprMatrix"], "exprMatrix.tsv") )
    #conf["exprMatrix"] = "exprMatrix.tsv"

    if "description" in conf:
        fname = makeAbs(inDir, conf["description"])
        if not isfile(fname):
            logging.error("%s referred to in config file does not exist, skipping" % fname)
            conf["description"] = None
        else:
            copyFiles.append( (fname, "description.html") )
            conf["description"] = "description.html"

    if "makeDoc" in conf:
        fname = makeAbs(inDir, conf["makeDoc"])
        if not isfile(fname):
            logging.error("%s referred to in config file does not exist, skipping" % fname)
            conf["makeDoc"] = None
        else:
            copyFiles.append( (fname, "makeDoc.html") )
            conf["makeDoc"] = "makeDoc.html"

    #if "colors" in conf:
        #fname = makeAbs(inDir, conf["colors"])
        #if not isfile(fname):
            #logging.error("%s referred to in config file does not exist, skipping" % fname)
            #conf["colors"] = None
        #else:
            #copyFiles.append( (fname, "colors.tsv") )
            #conf["colors"] = "colors.tsv"

    #newCoords = []
    #for coordIdx, (origFname, label) in enumerate(conf["coords"]):
        #newFname = "coords%d" % coordIdx
        #copyFiles.append( (origFname, newFname) )
        #newCoords.append( (newFname, label) )
    #conf["coords"] = newCoords

    #newMarkers = []
    #for coordIdx, (origFname, label) in enumerate(conf["markers"]):
        #newFname = "markers%d" % coordIdx
        #copyFiles.append( (origFname, newFname) )
        #newMarkers.append( (newFname, label) )
    #conf["markers"] = newMarkers

    return conf, copyFiles

def findInputFiles(options):
    """ find all input files and return them
    returns these:
    metaFname = file name meta data
    matrixFname = file name expression matrix
    coordFnames = list of (filename, label)
    markerFnames = list of (filename, label)
    filesToCopy = list of (origFname, copyToFname)
    """
    if options.inConf:
        conf = loadConfig(options.inConf)
        inDir = dirname(options.inConf)
    else:
        conf = guessConfig(options)

    # command line options can override the config file settings
    #if options.meta:
        #conf["meta"] = options.meta
    #if options.matrix:
        #conf["exprMatrix"] = options.matrix
    #if options.coords:
        #conf["coords"] = options.coords
    #if options.markers:
        #conf["markers"] = options.markers

    conf, copyFiles = mapFilenames(inDir, conf)

    return inDir, conf, copyFiles

def makeAbs(inDir, fname):
    " return absolute path of fname under inDir "
    return abspath(join(inDir, fname))

def makeAbsDict(inDir, dicts):
    " given list of dicts with key 'file', make paths absolute "
    for d in dicts:
        d["file"] = makeAbs(inDir, d["file"])
    return dicts

def parseTsvColumn(fname, colName):
    " parse a tsv file and return column as a pair (values, assignment row -> index in values) "
    logging.info("Parsing column %s from %s" % (colName, fname))
    vals = parseOneColumn(fname, colName)

    newVals = []
    valToInt = {}
    maxIdx = -1
    for v in vals:
        if v not in valToInt:
            maxIdx+=1
            valToInt[v] = maxIdx
        idx = valToInt[v]
        newVals.append(idx)

    intToVal = {v: k for k, v in valToInt.iteritems()} # inverse key/val in dict

    valArr = []
    for i in range(0, maxIdx+1):
        valArr.append(intToVal[i])

    return newVals, valArr

def makeMids(xVals, yVals, labelVec, labelVals):
    """
    calculate the positions (centers) for the cluster labels
    given a coord list and a vector of the same size with the label indeces, return a list of [x, y, coordLabel] 
    """
    logging.info("Calculating cluster midpoints")
    assert(len(xVals)==len(labelVec)==len(yVals))

    # prep the arrays
    clusterXVals = []
    clusterYVals = []
    for i in range(len(labelVals)):
        clusterXVals.append([])
        clusterYVals.append([])
    assert(len(clusterXVals)==len(labelVals))

    # sort the coords into separate arrays, one per cluster
    for i in range(len(labelVec)):
        #for (x, y), clusterIdx in zip(coords, labelVec):
        clusterIdx = labelVec[i]
        clusterXVals[clusterIdx].append(xVals[i])
        clusterYVals[clusterIdx].append(yVals[i])

    midInfo = []
    for clustIdx, xList in enumerate(clusterXVals):
        yList = clusterYVals[clustIdx]
        # get the midpoint
        midX = sum(xList) / float(len(xList))
        midY = sum(yList) / float(len(yList))

        # take only the best 70% of the points closest to the midpoints
        xyDist = []
        for x, y in zip(xList, yList):
            dist = math.sqrt((x-midX)**2+(y-midY)**2)
            xyDist.append( (dist, x, y) )
        xyDist.sort()
        xyDistBest = xyDist[:int(0.7*len(xyDist))]

        # now recalc the midpoint
        xSum = sum([x for dist, x, y in xyDistBest])
        ySum = sum([y for dist, x, y in xyDistBest])
        midX = xSum / float(len(xyDistBest))
        midY = ySum / float(len(xyDistBest))

        clusterName = labelVals[clustIdx]
        midInfo.append([midX, midY, clusterName])
    return midInfo

def addDataset(inDir, conf, fileToCopy, outDir, options):
    " write config to outDir and copy over all files in fileToCopy "
    # keep a copy of the original config
    confName = join(outDir, "origConf.json")
    json.dump(conf, open(confName, "w"))

    for src, dest in fileToCopy:
        logging.info("Copying %s -> %s" % (src, dest))
        shutil.copy(src, join(outDir, dest))

    matrixFname = makeAbs(inDir, conf["exprMatrix"])
    metaFname = makeAbs(inDir, conf["meta"])
    coordFnames = makeAbsDict(inDir, conf["coords"])
    markerFnames = makeAbsDict(inDir, conf["markers"])

    skipLog = conf["matrixIsLog"]
    colorFname = conf.get("colors")

    # these don't exist / are not needed in the output json file
    del conf["meta"]
    del conf["exprMatrix"]
    del conf["matrixIsLog"]
    del conf["colors"]

    # index the meta data
    metaDir = join(outDir, "metaFields")
    makeDir(metaDir)
    fixedMeta = join(outDir, "meta.tsv")
    sampleNames, needFilterMatrix = metaReorder(matrixFname, metaFname, fixedMeta)
    conf["sampleCount"] = len(sampleNames)
    conf = metaToBin(metaFname, colorFname, metaDir, conf)
    indexMeta(metaFname, join(outDir, "meta.index"))

    # index the expression matrix
    geneToSym = readGeneToSym(options.symTable)

    myMatrixFname = join(outDir, "exprMatrix.tsv")
    if matrixFname.endswith(".gz"):
        myMatrixFname += ".gz"

    binMatFname = join(outDir, "exprMatrix.bin")
    matrixIndexFname = join(outDir, "exprMatrixOffsets.json")
    #indexMatrix(matrixFname, geneToSym, matrixIndexFname)

    if options.quick and isfile(matrixIndexFname):
        logging.info("quick-mode: Not rebuilding matrix index")
    else:
        matrixToBin(matrixFname, geneToSym, binMatFname, matrixIndexFname, skipLog)

    if options.quick and isfile(myMatrixFname):
        logging.info("quick-mode: Not rebuilding expression matrix")
    else:
        copyMatrix(matrixFname, myMatrixFname, sampleNames, needFilterMatrix)

    # convert the coordinates
    newCoords = []
    for coordIdx, coordInfo in enumerate(coordFnames):
        coordFname = coordInfo["file"]
        coordLabel = coordInfo["shortLabel"]
        coords = parseScaleCoordsAsDict(coordFname, options.useTwoBytes)
        #coordLabel = basename(coordFname).split(".")[0]
        coordName = "coords_%d" % coordIdx
        #coordName = cleanString(coordLabel)
        coordDir = join(outDir, "coords", coordName)
        makeDir(coordDir)
        coordBin = join(coordDir, "coords.bin")
        coordJson = join(coordDir, "coords.json")
        coordInfo = OrderedDict()
        coordInfo["name"] = coordName
        coordInfo["shortLabel"] = coordLabel
        coordInfo, xVals, yVals = writeCoords(coords, sampleNames, coordBin, coordJson, options.useTwoBytes, coordInfo)
        newCoords.append( coordInfo )
        conf["coords"] = newCoords

        if "labelField" in conf:
            clusterLabelField = conf["labelField"]
            labelVec, labelVals = parseTsvColumn(metaFname, clusterLabelField)
            clusterMids = makeMids(xVals, yVals, labelVec, labelVals)

            midFname = join(coordDir, "clusterLabels.json")
            midFh = open(midFname, "w")
            json.dump(clusterMids, midFh, indent=2)
            logging.info("Wrote cluster labels to %s" % midFname)


    # convert the markers
    newMarkers = []
    for markerIdx, markerInfo in enumerate(markerFnames):
        markerFname = markerInfo["file"]
        markerLabel = markerInfo["shortLabel"]

        markerName = "markers_%d" % coordIdx # use sha1 of input file ?
        markerDir = join(outDir, "markers", markerName)
        makeDir(markerDir)

        splitMarkerTable(markerFname, geneToSym, markerDir)

        newMarkers.append( {"name" : markerName, "shortLabel" : markerLabel})
    conf["markers"] = newMarkers

    # write dataset summary info
    descJson = join(outDir, "dataset.json")
    descJsonFh = open(descJson, "w")
    json.dump(conf, descJsonFh, indent=2)
    logging.info("Wrote %s" % descJson)

def main():
    args, options = parseArgs()

    if options.test:
        testMetaIndex(args[0])
        sys.exit(0)

    if options.inConf is None:
        errAbort("You have to specify at least an input config file.")
    if options.outDir is None:
        errAbort("You have to specify at least the output directory.")

    inDir, conf, filesToCopy = findInputFiles(options)

    outDir = options.outDir


    #if outDir is None or metaFname is None or matrixFname is None or len(coordFnames)==0:
        #errAbort("You have to specify at least a meta file, an expression matrix, a coordinate file  and one output directory")
    if outDir is None:
        errAbort("You have to specify at least the output directory.")

    datasetDir = join(outDir, conf["name"])
    makeDir(datasetDir)

    addDataset(inDir, conf, filesToCopy, datasetDir, options)

main()
