#!/usr/bin/env python2

import logging, sys, optparse, struct, json, os, string, shutil, gzip, re, unicodedata
from collections import defaultdict, namedtuple, OrderedDict
from os.path import join, basename, dirname, isfile, isdir

# directory to static data files, e.g. gencode tables
dataDir = join(dirname(__file__), "static")

# ==== functions =====
    
def parseArgs():
    " setup logging, parse command line arguments and options. -h shows auto-generated help page "
    parser = optparse.OptionParser("usage: %prog [options] -i dataset.conf -o outputDir - add a dataset to the single cell viewer")

    parser.add_option("-d", "--debug", dest="debug", action="store_true",
        help="show debug messages")
    #parser.add_option("-m", "--meta", dest="meta", action="store",
        #help="meta data tsv file, aka sample sheet. One row per sample, first row has headers, first column has sample name."
        #)
    #parser.add_option("-e", "--matrix", dest="matrix", action="store",
        #help="expression matrix file, one gene per row, one sample per column. First column has gene identifiers (Ensembl or symbol), First row has sample names. ")
    #parser.add_option("-c", "--coords", dest="coords", action="append", help="tab-sep table with cell coordinates, format: metaId, x, y. Can be specified multiple times, if you have multiple coordinate files.")

    parser.add_option("-i", "--inConf", dest="inConf", action="store",
        help="a dataset.conf file that specifies labels and all input files")
    parser.add_option("-o", "--outDir", dest="outDir", action="store", help="output directory")
    parser.add_option("-s", "--symTable",
        dest="symTable",
        action="store", help="use a table (transId, geneId, symbol) to convert transcript or gene identifiers in the expression matrix to symbols, default %default. Specify '-s none' if the input expression file is already using gene symbols.",
        default=join(dataDir, "gencode22.ens79-80.tab"))

    parser.add_option("-t", "--useTwoBytes",
        dest="useTwoBytes",
        action="store_true", help="use 2-byte-integers for the coordinates, default is to use four bytes")

    #parser.add_option("-f", "--file", dest="file", action="store", help="run on file") 
    #parser.add_option("", "--test", dest="test", action="store_true", help="do something") 
    (options, args) = parser.parse_args()

    #if args==[]:
        #parser.print_help()
        #exit(1)

    if options.debug:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    return args, options

def makeDir(outDir):
    if not isdir(outDir):
        logging.info("Creating %s" % outDir)
        os.makedirs(outDir)

def errAbort(msg):
        logging.error(msg)
        sys.exit(1)

def lineFileNextRow(inFile):
    """
    parses tab-sep file with headers in first line
    yields collection.namedtuples
    strips "#"-prefix from header line
    """

    if isinstance(inFile, str):
        fh = openFile(inFile)
    else:
        fh = inFile

    line1 = fh.readline()
    line1 = line1.strip("\n").lstrip("#")
    headers = line1.split("\t")
    headers = [re.sub("[^a-zA-Z0-9_]","_", h) for h in headers]
    headers = [re.sub("^_","", h) for h in headers] # remove _ prefix
    #headers = [x if x!="" else "noName" for x in headers]
    if headers[0]=="": # R does not name the first column by default
        headers[0]="rowName"

    if "" in headers:
        logging.error("Found empty cells in header line of %s" % inFile)
        logging.error("This often happens with Excel files. Make sure that the conversion from Excel was done correctly. Use cut -f-lastColumn to fix it.")
        assert(False)

    filtHeads = []
    for h in headers:
        if h[0].isdigit():
            filtHeads.append("x"+h)
        else:
            filtHeads.append(h)
    headers = filtHeads


    Record = namedtuple('tsvRec', headers)
    for line in fh:
        if line.startswith("#"):
            continue
        line = line.decode("latin1")
        # skip special chars in meta data and keep only ASCII
        line = unicodedata.normalize('NFKD', line).encode('ascii','ignore')
        line = line.rstrip("\n").rstrip("\r")
        fields = string.split(line, "\t", maxsplit=len(headers)-1)
        try:
            rec = Record(*fields)
        except Exception as msg:
            logging.error("Exception occured while parsing line, %s" % msg)
            logging.error("Filename %s" % fh.name)
            logging.error("Line was: %s" % line)
            logging.error("Does number of fields match headers?")
            logging.error("Headers are: %s" % headers)
            raise Exception("header count: %d != field count: %d wrong field count in line %s" % (len(headers), len(fields), line))
        yield rec

def parseIntoColumns(fname):
    ifh = open(fname)
    sep = "\t"
    headers = ifh.readline().rstrip("\n").rstrip("\r").split(sep)

    columns = []
    for h in headers:
        columns.append([])

    for line in ifh:
        row = line.rstrip("\n").rstrip("\r").split(sep)
        for i in range(len(headers)):
            columns[i].append(row[i])
    return zip(headers, columns)

def openFile(fname):
    if fname.endswith(".gz"):
        fh = gzip.open(fname)
    else:
        fh = open(fname)
    return fh

def parseDict(fname):
    """ parse text file in format key<tab>value and return as dict key->val """
    d = {}

    fh = openFile(fname)

    for line in fh:
        key, val = line.rstrip("\n").split("\t")
        d[key] = val
    return d

def readGeneToSym(fname):
    " given a file with geneId,symbol return a dict geneId -> symbol. Strips anything after . in the geneId "
    if fname.lower()=="none":
        return None

    logging.info("Reading gene,symbol mapping from %s" % fname)

    # Jim's files and CellRanger files have no headers, they are just key-value
    line1 = open(fname).readline()
    if "geneId" not in line1:
        d = parseDict(fname)
    # my new gencode tables contain a symbol for ALL genes
    elif line1=="transcriptId\tgeneId\tsymbol":
        for row in lineFileNextRow(fname):
            if row.symbol=="":
                continue
            d[row.geneId.split(".")[0]]=row.symbol
    # my own files have headers
    else:
        d = {}
        for row in lineFileNextRow(fname):
            if row.symbol=="":
                continue
            d[row.geneId.split(".")[0]]=row.symbol
    return d

def getDecilesList(values):
    """ given a list of values, return the 11 values that define the 10 ranges for the deciles 
    >>> l = [-4.059,-3.644,-3.184,-3.184,-3.184,-3.059,-2.943,-2.396,-2.322,-2.252,-2.252,-2.252,-2.12,-2.12,-1.943,-1.943,-1.69,-1.556,-1.322,-1.184,-1.12,-0.862,-0.862,-0.837,-0.837,-0.667,-0.644,-0.535,-0.454,-0.234,-0.184,0.084,0.084,0.151,0.299,0.31,0.444,0.485,0.575,0.632,0.66,0.748,0.824,1.043,1.098,1.176,1.239,1.356,1.411,1.521,1.526,1.609,1.748,1.77,1.832,1.864,1.88,2.081,2.094,2.242,2.251,2.331,2.376,2.664,2.718,2.787,2.858,2.928,2.978,3.011,3.093,3.144,3.157,3.245,3.352,3.444,3.462,3.479,3.609,3.699,3.714,3.795,3.811,3.857,3.871,3.903,3.914,3.983,3.985,3.986,4.006,4.056,4.063,4.156,4.179,4.221,4.35,4.352,4.361,4.372,4.38,4.427,4.432,4.447,4.45,4.459,4.516,4.521,4.527,4.611,4.636,4.644,4.659,4.662,4.81,4.866,4.882,4.902,4.916,4.918,5.01,5.018,5.02,5.133,5.179,5.186,5.205,5.218,5.245,5.25,5.262,5.263,5.365,5.374,5.412,5.421,5.437,5.45,5.453,5.484,5.501,5.527,5.527,5.534,5.561,5.566,5.567,5.624,5.646,5.662,5.691,5.705,5.706,5.712,5.87,5.909,5.912,5.92,5.978,5.978,6.012,6.086,6.086,6.106,6.114,6.155,6.168,6.171,6.179,6.221,6.287,6.317,6.324,6.354,6.364,6.385,6.389,6.397,6.427,6.439,6.49,6.513,6.517,6.521,6.557,6.578,6.579,6.648,6.703,6.756,6.887,6.953,7.042,7.155,7.194,7.21,7.225,7.249,7.254,7.291,7.382,7.397,7.504,7.603,7.65,7.861,8.524]
    >>> getDecilesList(l)
    [-4.059, -1.12, 0.748, 2.331, 3.811, 4.447, 5.133, 5.561, 6.114, 6.578, 8.524]
    """
    if len(values)==0:
        return None

    values.sort()
    binSize = float(len(values)-1) / 10.0; # width of each bin, in number of elements, fractions allowed

    # get deciles from the list of sorted values
    deciles = []
    pos = 0
    for i in range(11): # 10 bins means that we need 11 ranges
        pos = int (binSize * i)
        if pos > len(values): # this should not happen, but maybe it can, due to floating point issues?
            logging.warn("decile exceeds 10, binSize %d, i %d, len(values) %d" % (binSize, i, len(values)))
            pos = len(values)
        deciles.append ( values[pos] )
    return deciles

def bytesAndFmt(x):
    " how many bytes do we need to store x values and what is the sprintf format string for it? "
    if x > 65535:
        assert(False) # field with more than 65k elements or high numbers? Weird meta data.

    if x > 255:
        return 2, "<H"
    else:
        return 1, "<B"

def discretizeField(numVals, fieldMeta):
    " given a list of values, add attributes to fieldMeta that describe the deciles "
    deciles = getDecilesList(numVals)
    binCounts = [0]*10
    newVals = []
    for x in numVals:
        binIdx = findBin(deciles, x)
        newVals.append(binIdx)
        binCounts[binIdx]+=1

    fieldMeta["deciles"] = deciles
    fieldMeta["binCounts"] = binCounts
    fieldMeta["byteCount"] = 1
    fieldMeta["_fmt"] = "<B"
    return newVals, fieldMeta

def getFieldMeta(valList):
    """ given a list of strings, determine if they're all int, float or
    strings. Return fieldMeta, as dict, and a new valList, with the correct python type
    - 'type' can be: 'int', 'float', 'enum' or 'uniqueString'
    - if int or float: 'deciles' is a list of the deciles
    - if uniqueString: 'maxLen' is the length of the longest string
    - if enum: 'values' is a list of all possible values
    """
    intCount = 0
    floatCount = 0
    valCounts = defaultdict(int)
    maxVal = 0
    for val in valList:
        fieldType = "string"
        try:
            newVal = int(val)
            intCount += 1
            floatCount += 1
            maxVal = max(newVal, val)
        except:
            try:
                newVal = float(val)
                floatCount += 1
                maxVal = max(newVal, val)
            except:
                pass

        valCounts[val] += 1

    valToInt = None

    fieldMeta = {}
    if floatCount==len(valList) and intCount!=len(valList) and len(valCounts) > 10:
        # convert to decile index
        numVals = [float(x) for x in valList]
        newVals, fieldMeta = discretizeField(numVals, fieldMeta)
        fieldMeta["type"] = "float"
        fieldMeta["maxVal"] = maxVal

    elif intCount==len(valList):
        # convert to decile index
        numVals = [int(x) for x in valList]
        newVals, fieldMeta = discretizeField(numVals, fieldMeta)
        fieldMeta["type"] = "int"
        fieldMeta["maxVal"] = maxVal

    elif len(valCounts)==len(valList):
        fieldMeta["type"] = "uniqueString"
        maxLen = max([len(x) for x in valList])
        fieldMeta["byteCount"] = maxLen
        fieldMeta["_fmt"] = "%ds" % (maxLen+1)
        newVals = valList

    else:
        # convert to enum index
        fieldMeta["type"] = "enum"
        valArr = list(valCounts.keys())
        fieldMeta["valCounts"] = valCounts
        fieldMeta["byteCount"], fieldMeta["_fmt"] = bytesAndFmt(len(valArr))
        valToInt = dict([(y,x) for (x,y) in enumerate(valArr)]) # dict with value -> index in valArr
        newVals = [valToInt[x] for x in valList]

    #fieldMeta["valCount"] = len(valList)
    fieldMeta["diffValCount"] = len(valCounts)

    return fieldMeta, newVals

def writeNum(col, packFmt, ofh):
    " write a list of numbers to a binary file "

def findBin(ranges, val):
    """ given an array of values, find the index i where ranges[i] < val < ranges[i+1]
    This is a dumb brute force implementation - use binary search?
    """
    for i in range(0, len(ranges)):
        if ((val >= ranges[i]) and (val <= ranges[i+1])):
            return i

def cleanString(s):
    " returns only alphanum characters in string s "
    newS = []
    for c in s:
        if c.isalnum():
            newS.append(c)
    return "".join(newS)

def metaToBin(fname, outDir, datasetInfo):
    """ convert meta table to binary files. outputs fields.json and one binary file per field. 
    adds names of metadata fields to datasetInfo and returns datasetInfo
    """
    makeDir(outDir)

    colData = parseIntoColumns(fname)

    jsonName = join(outDir, "fields.json")
    jsonFh = open(jsonName, "w")

    fieldInfo = []
    for colIdx, (fieldName, col) in enumerate(colData):
        print("Field %d = %s" % (colIdx, fieldName))
        cleanFieldName = cleanString(fieldName)
        binName = join(outDir, fieldName+".bin")

        fieldMeta, binVals = getFieldMeta(col)
        fieldMeta["name"] = cleanFieldName
        fieldMeta["label"] = fieldName
        fieldType = fieldMeta["type"]

        packFmt = fieldMeta["_fmt"]

        # write the binary file
        binFh = open(binName, "wb")
        if fieldMeta["type"]!="uniqueString":
            for x in binVals:
                binFh.write(struct.pack(packFmt, x))
        else:
            for x in col:
                binFh.write("%s\n" % x)
        binFh.close()

        # write the json file
        del fieldMeta["_fmt"]
        #jsonStr = json.dumps(fieldMeta,indent=2, sort_keys=True)
        #jsonFh.write(jsonStr)
        fieldInfo.append(fieldMeta)
        print(("Type: %(type)s, %(diffValCount)d different values, %(byteCount)d bytes per element" % fieldMeta))

    datasetInfo["metaFields"] = fieldInfo
    #jsonFh.close()
    return datasetInfo

def iterLineOffsets(ifh):
    """ parse a text file and yield tuples of (line, startOffset, endOffset).
    endOffset does not include the newline, but the newline is not stripped from line.
    """
    line = True
    start = 0
    while line!='':
       line = ifh.readline()
       end = ifh.tell()-1
       if line!="":
           yield line, start, end
       start = ifh.tell()

def indexMatrix(fname, geneToSym, jsonFname):
    """ index a matrix with one gene per line and write json with a dict
        gene symbol -> (file offset, line length)
    """
    logging.info("Indexing line offsets of %s into %s" % (fname, jsonFname))
    ifh = open(fname)
    geneToOffset = {}
    skipIds = 0
    lineNo = 0
    for line, start, end in iterLineOffsets(ifh):
        if start == 0:
            symbol = "_header"
        else:
            geneId, _ = string.split(line, "\t", 1)
            if geneToSym is None:
                symbol = geneId
            else:
                symbol = geneToSym.get(geneId)
                if symbol is None:
                    skipIds += 1
                    continue
        lineLen = end - start
        assert(lineLen!=0)
        if symbol in geneToOffset:
            logging.warn("Gene %s/%s is duplicated in matrix, using only first occurence" % (geneId, symbol))
            continue
        geneToOffset[symbol] = (start, lineLen)
        lineNo += 1

    if len(geneToOffset)==1:
        errAbort("No genes could be mapped to symbols in the input file. Are you sure these are Ensembl IDs? If they are already gene symbols, re-run with '-s none'. One example gene ID is %s." % geneId)

    if skipIds!=0:
        logging.warn("Could not resolve %d genes from Ensembl ID to gene symbol" % skipIds)

    ofh = open(jsonFname, "w")
    json.dump(geneToOffset, ofh)
    ofh.close()

def indexMeta(fname, outFname):
    """ index a tsv by its first field. Writes binary data to outFname.
        binary data is (offset/4 bytes, line length/2 bytes)
    """
    ofh = open(outFname, "wb")
    logging.info("Indexing meta file %s to %s" % (fname, outFname))
    ifh = open(fname)
    headerDone = False
    for line, start, end in iterLineOffsets(ifh):
        if not headerDone:
            headerDone = True
            continue

        field1, _ = string.split(line, "\t", 1)
        lineLen = end - start
        assert(lineLen!=0)
        assert(lineLen<65535) # meta data line cannot be longer than 2 bytes
        ofh.write(struct.pack("<L", start))
        ofh.write(struct.pack("<H", lineLen))
    ofh.close()

#def testMetaIndex():
#    # test meta index
#    fh = open(join(outDir, "meta/metaOffsets.bin"))
#    fh.seek(10*6)
#    o = fh.read(4)
#    s = fh.read(2)
#    offset = struct.unpack("<L", o)
#    l = struct.unpack("<H", s)
#    print offset, l
#
#    fh = open(join(outDir, "meta/meta.tsv"))
#    fh.seek(offset[0])
#    print fh.read(l[0])

# ----------- main --------------

def parseScaleCoordsAsDict(fname, useTwoBytes):
    """ parse tsv file in format cellId, x, y and return as dict (cellId, x, y)
    Flip the y coordinates to make it more look like plots in R, for people transitioning from R.
    """
    logging.info("Parsing coordinates from %s" % fname)
    coords = []
    maxY = 0
    minX = 2^32
    minY = 2^32
    maxX = -2^32
    maxY = -2^32
    skipCount = 0
    for row in lineFileNextRow(fname):
        assert(len(row)==3) # coord file has to have three rows (cellId, x, y), we ignore the headers
        cellId = row[0]
        x = float(row[1])
        y = float(row[2])
        minX = min(x, minX)
        minY = min(y, minY)
        maxX = max(x, maxX)
        maxY = max(y, maxY)
        coords.append( (cellId, x, y) )

    if useTwoBytes:
        scaleX = (maxX-minX)/65536
        scaleY = (maxY-minY)/65536
        topY = 65535

    # flip coords on the Y axis, possibly scale to 16 bits
    newCoords = {}
    for cellId, x, y in coords:
        if useTwoBytes:
            newX = int(scaleX * (maxX - x))
            newY = int(65536 - (scaleY * (maxY - y)))
        else:
            newY = maxY - y # just flip

        newCoords[cellId] = (x, newY)

    return newCoords

def metaReorder(matrixFname, metaFname, fixedMetaFname):
    " check and reorder the meta data, has to be in the same order as the expression matrix, write to fixedMetaFname "
    logging.info("Checking and reordering meta data to %s" % fixedMetaFname)
    matrixSampleNames = open(matrixFname).readline().rstrip("\n").rstrip("\r").split("\t")[1:]
    metaSampleNames = [string.split(line, "\t", 1)[0] for line in open(metaFname)][1:]

    # check that there is a 1:1 sampleName relationship
    mat = set(matrixSampleNames)
    meta = set(metaSampleNames)
    if len(meta)!=len(metaSampleNames):
        logging.error("The meta data table contains a duplicate sample name")
        sys.exit(1)

    if len(mat.intersection(meta))==0:
        logging.error("Meta data and expression matrix have no single sample name in common")
        sys.exit(1)

    matNotMeta = meta - mat
    metaNotMat = mat - meta
    stop = False
    doFilter = False
    if len(metaNotMat)!=0:
        logging.warn("%d samples names are in the meta data, but not in the expression matrix. Examples: %s" % (len(metaNotMat), list(metaNotMat)[:10]))
        logging.warn("These samples will be dropped from the expression matrix")
        matrixSampleNames = metaSampleNames
        doFilter = True

    if len(matNotMeta)!=0:
        logging.warn("%d samples names are in the expression matrix, but not in the meta data. Examples: %s" % (len(matNotMeta), list(matNotMeta)[:10]))
        logging.warn("These samples will be dropped from the meta data")

    #if stop:
        #sys.exit(1)

    logging.info("Data contains %d samples/cells" % len(matrixSampleNames))

    # slurp in the whole meta data
    ofh = open(fixedMetaFname, "w")
    metaToLine = {}
    for lNo, line in enumerate(open(metaFname)):
        if lNo==0:
            ofh.write(line)
            continue
        row = line.rstrip("\n").rstrip("\r").split("\t")
        metaToLine[row[0]] = line

    # and spit it out in the right order
    for matrixName in matrixSampleNames:
        ofh.write(metaToLine[matrixName])
    ofh.close()

    return matrixSampleNames, doFilter

def writeCoords(coords, sampleNames, coordBin, coordJson, useTwoBytes):
    " write coordinates given as a dictionary to coordBin and coordJson, in the order of sampleNames "
    logging.info("Writing coordinates to %s and %s" % (coordBin, coordJson))
    binFh = open(coordBin, "wb")
    minX = 2^32
    minY = 2^32
    maxX = -2^32
    maxY = -2^32
    for sampleName in sampleNames:
        coordTuple = coords.get(sampleName)
        if coordTuple is None:
            errAbort("sample name %s is in coordinate file but not in matrix nor meta data")
        x, y = coordTuple
        minX = min(x, minX)
        minY = min(y, minY)
        maxX = max(x, maxX)
        maxY = max(y, maxY)

        if useTwoBytes:
            binFh.write(struct.pack("<H", x))
            binFh.write(struct.pack("<H", y))
        else:
            binFh.write(struct.pack("<f", x))
            binFh.write(struct.pack("<f", y))
    
    coordInfo = {}
    coordInfo["minX"] = minX
    coordInfo["maxX"] = maxX
    coordInfo["minY"] = minY
    coordInfo["maxY"] = maxY
    if useTwoBytes:
        coordInfo["bytes"] = 2
    else:
        coordInfo["bytes"] = 4

def copyMatrix(inFname, outFname, filtSampleNames, doFilter):
    " copy matrix. If doFilter is true: keep only the samples in filtSampleNames"
    if not doFilter:
        logging.info("Copying %s to %s" % (inFname, outFname))
        shutil.copy(inFname, outFname)
        return

    sep = "\t"

    logging.info("Copying %s to %s, keeping only the %d columns with a sample name in the meta data" % (inFname, outFname, len(filtSampleNames)))
    ifh = open(inFname)
    headLine = ifh.readline()
    headers = headLine.rstrip("\n").rstrip("\r").split(sep)

    keepFields = set(filtSampleNames)
    keepIdx = [0]
    for i, name in enumerate(headers):
        if name in keepFields:
            keepIdx.append(i)

    ofh = open(outFname, "w")
    for line in ifh:
        row = line.rstrip("\n").rstrip("\r").split(sep)
        newRow = []
        for idx in keepIdx:
            newRow.append(row[idx])
        ofh.write("\t".join(newRow))
        ofh.write("\n")
    ofh.close()

def convIdToSym(geneToSym, geneId):
    if geneToSym is None:
        return geneId
    else:
        return geneToSym[geneId]

def splitMarkerTable(filename, geneToSym, outDir):
    " split .tsv on first field and create many files in outDir with the non-first columns. Also map geneIds to symbols. "
    if filename is None:
        return
    logging.info("Splitting cluster markers from %s into directory %s" % (filename, outDir))
    #logging.debug("Splitting %s on first field" % filename)
    ifh = open(filename)

    headers = ifh.readline().rstrip("\n").split('\t')
    otherHeaders = headers[2:]

    data = defaultdict(list)
    for line in ifh:
        row = line.rstrip("\n").split('\t')
        markerName = row[0]
        geneId = row[1]
        scoreVal = float(row[2])
        otherFields = row[3:]

        geneSym = convIdToSym(geneToSym, geneId)

        newRow = []
        newRow.append(geneId)
        newRow.append(geneSym)
        newRow.append(scoreVal)
        newRow.extend(otherFields)

        data[markerName].append(newRow)

    newHeaders = ["id", "symbol"]
    newHeaders.extend(otherHeaders)

    fileCount = 0
    for markerName, rows in data.iteritems():
        #rows.sort(key=operator.itemgetter(2), reverse=True) # rev-sort by score (fold change)
        markerName = markerName.replace("/","_")
        outFname = join(outDir, markerName+".tsv")
        logging.debug("Writing %s" % outFname)
        ofh = open(outFname, "w")
        ofh.write("\t".join(newHeaders))
        ofh.write("\n")
        for row in rows:
            row[2] = "%0.3f" % row[2] # limit to 3 digits
            ofh.write("\t".join(row))
            ofh.write("\n")
        ofh.close()
        fileCount += 1
    logging.info("Wrote %d .tsv files into directory %s" % (fileCount, outDir))


def loadConfig(fname):
    " parse python in fname and return variables as dictionary "
    g = {}
    l = OrderedDict()
    execfile(fname, g, l)

    conf = l

    if not "coords" in conf:
        errAbort("The input configuration has to define the 'coords' statement")
    if not "meta" in conf:
        errAbort("The input configuration has to define the 'meta' statement")
    if not "exprMatrix" in conf:
        errAbort("The input configuration has to define the 'exprMatrix' statement")

    return conf

def guessConfig(options):
    " guess reasonable config options from arguments "
    conf = {}
    conf.name = dirname(options.matrix)

    #if options.inDir:
        #inDir = options.inDir
        #metaFname = join(inDir, "meta.tsv")
        #matrixFname = join(inDir, "exprMatrix.tsv")
        #coordFnames = [join(inDir, "tsne.coords.tsv")]
        #markerFname = join(inDir, "markers.tsv")
        #if isfile(markerFname):
            #markerFnames = [markerFname]
        #else:
            #markerFnames = None
#
        #acronymFname = join(inDir, "acronyms.tsv")
        #if isfile(acronymFname):
            #otherFiles["acronyms"] = [acronymFname]
#
        #markerFname = join(inDir, "markers.tsv")
        #if isfile(acronymFname):
            #otherFiles["markerLists"] = [markerFname]
    return conf

def mapFilenames(conf):
    # replace file names in config files with file names in the output directory
    copyFiles = []
    #copyFiles.append( (conf["meta"], "meta.tsv") )
    #conf["meta"] = "meta.tsv"

    #copyFiles.append( (conf["exprMatrix"], "exprMatrix.tsv") )
    #conf["exprMatrix"] = "exprMatrix.tsv"

    if "descHtml" in conf:
        fname = conf["descHtml"]
        if not isfile(fname):
            logging.error("%s referred to in config file does not exist, skipping" % fname)
            conf["description"] = None
        else:
            copyFiles.append( (fname, "description.html") )
            conf["description"] = "description.html"

    if "makeDoc" in conf:
        fname = conf["makeDoc"]
        if not isfile(fname):
            logging.error("%s referred to in config file does not exist, skipping" % fname)
            conf["makeDoc"] = None
        else:
            copyFiles.append( (fname, "makeDoc.html") )
            conf["makeDoc"] = "makeDoc.html"

    if "colors" in conf:
        fname = conf["colors"]
        if not isfile(fname):
            logging.error("%s referred to in config file does not exist, skipping" % fname)
            conf["colors"] = None
        else:
            copyFiles.append( (fname, "colors.tsv") )
            conf["colors"] = "colors.tsv"

    #newCoords = []
    #for coordIdx, (origFname, label) in enumerate(conf["coords"]):
        #newFname = "coords%d" % coordIdx
        #copyFiles.append( (origFname, newFname) )
        #newCoords.append( (newFname, label) )
    #conf["coords"] = newCoords

    #newMarkers = []
    #for coordIdx, (origFname, label) in enumerate(conf["markers"]):
        #newFname = "markers%d" % coordIdx
        #copyFiles.append( (origFname, newFname) )
        #newMarkers.append( (newFname, label) )
    #conf["markers"] = newMarkers

    return conf, copyFiles

def findInputFiles(options):
    """ find all input files and return them
    returns these:
    metaFname = file name meta data
    matrixFname = file name expression matrix
    coordFnames = list of (filename, label)
    markerFnames = list of (filename, label)
    filesToCopy = list of (origFname, copyToFname)
    """
    if options.inConf:
        conf = loadConfig(options.inConf)
    else:
        conf = guessConfig(options)

    # command line options can override the config file settings
    #if options.meta:
        #conf["meta"] = options.meta
    #if options.matrix:
        #conf["exprMatrix"] = options.matrix
    #if options.coords:
        #conf["coords"] = options.coords
    #if options.markers:
        #conf["markers"] = options.markers

    conf, copyFiles = mapFilenames(conf)

    return conf, copyFiles

def addDataset(conf, fileToCopy, outDir, options):
    " write config to outDir and copy over all files in fileToCopy "
    confName = join(outDir, "dataset.conf")
    json.dump(conf, open(confName, "w"))

    for src, dest in fileToCopy:
        logging.info("Copying %s -> %s" % (src, dest))
        shutil.copy(src, join(outDir, dest))

    matrixFname = conf["exprMatrix"]
    metaFname = conf["meta"]
    coordFnames = conf["coords"]
    markerFnames = conf["markers"]

    # index the meta data
    metaDir = join(outDir, "meta")
    makeDir(metaDir)
    fixedMeta = join(metaDir, "meta.tsv")
    sampleNames, needFilterMatrix = metaReorder(matrixFname, metaFname, fixedMeta)
    conf = metaToBin(metaFname, join(outDir, "meta"), conf)
    indexMeta(metaFname, join(outDir, "meta", "metaOffsets.bin"))

    # index the expression matrix
    geneToSym = readGeneToSym(options.symTable)
    myMatrixFname = join(outDir, "exprMatrix.tsv")
    matrixIndexFname = join(outDir, "exprMatrixOffsets.json")
    indexMatrix(matrixFname, geneToSym, matrixIndexFname)
    copyMatrix(matrixFname, myMatrixFname, sampleNames, needFilterMatrix)

    # convert the coordinates
    newCoords = []
    for coordIdx, coordInfo in enumerate(coordFnames):
        coordFname = coordInfo["file"]
        coordLabel = coordInfo["shortLabel"]
        coords = parseScaleCoordsAsDict(coordFname, options.useTwoBytes)
        #coordLabel = basename(coordFname).split(".")[0]
        coordDir = "coords_%d" % coordIdx
        #coordName = cleanString(coordLabel)
        coordDir = join(outDir, "coords", coordDir)
        makeDir(coordDir)
        coordBin = join(coordDir, "coords.bin")
        coordJson = join(coordDir, "coords.json")
        writeCoords(coords, sampleNames, coordBin, coordJson, options.useTwoBytes)
        newCoords.append( {"name" : coordDir, "shortLabel" : coordLabel})
    conf["coords"] = coordInfo

    # convert the markers
    newMarkers = []
    for markerIdx, markerInfo in enumerate(markerFnames):
        markerFname = markerInfo["file"]
        markerLabel = markerInfo["shortLabel"]

        markerName = "markers_%d" % coordIdx # use sha1 of input file ?
        markerDir = join(outDir, "markers", markerName)
        makeDir(markerDir)

        splitMarkerTable(markerFname, geneToSym, markerDir)

        newMarkers.append( {"name" : markerName, "shortLabel" : markerLabel})
    conf["markers"] = markerInfo

    conf["sampleCount"] = len(sampleNames)

    # write dataset summary info
    descJson = join(outDir, "dataset.json")
    descJsonFh = open(descJson, "w")
    json.dump(conf, descJsonFh, indent=2)
    logging.info("Wrote %s" % descJson)

def main():
    args, options = parseArgs()

    if options.inConf is None:
        errAbort("You have to specify at least an input config file.")
    if options.outDir is None:
        errAbort("You have to specify at least the output directory.")

    conf, filesToCopy = findInputFiles(options)

    outDir = options.outDir

    #if outDir is None or metaFname is None or matrixFname is None or len(coordFnames)==0:
        #errAbort("You have to specify at least a meta file, an expression matrix, a coordinate file  and one output directory")
    if outDir is None:
        errAbort("You have to specify at least the output directory.")

    datasetDir = join(outDir, conf["name"])
    makeDir(datasetDir)

    addDataset(conf, filesToCopy, datasetDir, options)

main()
